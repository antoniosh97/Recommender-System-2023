{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Colab execution only\n",
    "# \"\"\"\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# !git clone \"https://github.com/antoniosh97/Recommender-System-2023.git\"\n",
    "\n",
    "# path = Path('Recommender-System-2023/Implementation/3_LabReplication')\n",
    "# print(f\"Current path: \\n{path}\\nContent inside the folder:\\n{os.listdir(path)}\")\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\brend\\\\OneDrive\\\\Escritorio\\\\Postgrado\\\\RecSys_Project\\\\GitHub_repo\\\\Clone5 - copia\\\\Recommender-System-2023\\\\Implementation'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#====================== Import de librerias =====================#\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# !pip install wget\n",
    "# import wget\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from tqdm import tqdm, trange\n",
    "from IPython import embed\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys \n",
    "\n",
    "sampling_method = \"FM_RND_POP\"\n",
    "\n",
    "if not os.getcwd().split(os.sep)[-1] == \"Implementation\":\n",
    "    os.chdir(\"..\")\n",
    "execution_path = os.getcwd()\n",
    "execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "logs_base_dir = \"runs_\"+sampling_method\n",
    "os.environ[\"run_tensorboard\"] = logs_base_dir\n",
    "\n",
    "os.makedirs(f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}', exist_ok=True)\n",
    "tb_fm = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_FM/')\n",
    "tb_rnd = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_RANDOM/')\n",
    "tb_pop = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_POP/')\n",
    "\n",
    "def save_data_configuration(text):\n",
    "    save_data_dir = \"data_config_\" + sampling_method +\".txt\" \n",
    "    path = f'{execution_path}/{\"4_Modelling\"}/{save_data_dir}'\n",
    "    with open(path, \"a\") as data_file:\n",
    "        data_file.write(text+\"\\n\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some hyper-parameters\n",
    "hparams = {\n",
    "    'batch_size':64,\n",
    "    'num_epochs':12,\n",
    "    'hidden_size': 32,\n",
    "    'learning_rate':1e-4,\n",
    "}\n",
    "\n",
    "# we select to work on GPU if it is available in the machine, otherwise\n",
    "# will run on CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Definicion de valores de configuracion ============#\n",
    "\n",
    "min_reviews, min_usuarios = [6,6]\n",
    "col_names = {\"col_id_reviewer\": \"reviewerID\",\n",
    "             \"col_id_product\": \"asin\",\n",
    "             \"col_rating\": \"overall\",\n",
    "             \"col_unix_time\": \"unixReviewTime\",\n",
    "             \"col_timestamp\": \"timestamp\",\n",
    "             \"col_year\": \"year\"}\n",
    "\n",
    "csv_filename = execution_path/Path(\"3_DataPreparation/interactions_minR{}_minU{}.csv\".format(min_reviews,min_usuarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1477785600</td>\n",
       "      <td>2016-10-30 02:00:00</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10612</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1467244800</td>\n",
       "      <td>2016-06-30 02:00:00</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1454716800</td>\n",
       "      <td>2016-02-06 01:00:00</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1434844800</td>\n",
       "      <td>2015-06-21 02:00:00</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2523</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1420329600</td>\n",
       "      <td>2015-01-04 01:00:00</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asin  reviewerID  overall  unixReviewTime            timestamp  year\n",
       "0     0        9132      5.0      1477785600  2016-10-30 02:00:00  2016\n",
       "1     0       10612      5.0      1467244800  2016-06-30 02:00:00  2016\n",
       "2     0         257      1.0      1454716800  2016-02-06 01:00:00  2016\n",
       "3     0        4425      5.0      1434844800  2015-06-21 02:00:00  2015\n",
       "4     0        2523      4.0      1420329600  2015-01-04 01:00:00  2015"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin               6178\n",
       "reviewerID        14138\n",
       "overall               5\n",
       "unixReviewTime     3622\n",
       "timestamp          3622\n",
       "year                 15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_configuration(str(df.nunique()))\n",
    "df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset (TLOO strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"TLOO\"\n",
    "def split_train_test(data: np.ndarray,\n",
    "                     n_users: int, strategy) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Split and remove timestamp\n",
    "    train_x, test_x = [], []\n",
    "    for u in trange(n_users, desc='spliting train/test and removing timestamp...'):\n",
    "        user_data = data[data[:, 0] == u]\n",
    "        sorted_data = user_data[user_data[:, -1].argsort()]\n",
    "        if len(sorted_data) == 1:\n",
    "            train_x.append(sorted_data[0][:-1])\n",
    "        else:\n",
    "            if (strategy == \"TLOO\"):\n",
    "                train_x.append(sorted_data[:-1][:, :-1])\n",
    "                test_x.append(sorted_data[-1][:-1])\n",
    "            else:\n",
    "                # seleccionar uno random RLOO Random Leave One Out\n",
    "                idx = np.random.choice(np.arange(sorted_data.shape[0]), size=1) # devuelve size indices de la dimension 0\n",
    "                test_x.append(sorted_data[idx,:-1]) # añado el registro a test\n",
    "                sorted_data = np.delete(sorted_data, (idx), axis=0) # lo borramos de la lista que va a train            \n",
    "                train_x.append(sorted_data[:,:-1]) # añadimos el resto a train        \n",
    "    return np.vstack(train_x), np.stack(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14137\n",
      "6177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[      9132,          0,          5, 1477785600],\n",
       "       [     10612,          0,          5, 1467244800],\n",
       "       [       257,          0,          1, 1454716800],\n",
       "       ...,\n",
       "       [      9051,       6177,          5, 1530144000],\n",
       "       [      3412,       6177,          5, 1527465600],\n",
       "       [      9805,       6177,          5, 1527206400]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = df[[*col_names.values()][:3]].astype('int32').to_numpy()\n",
    "data = df[[*col_names.values()][:4]].astype('int32').to_numpy()\n",
    "print(max(data[:,0]))\n",
    "print(max(data[:,1]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim of users: 14138\n",
      "Dim of items: 20316\n",
      "Dims of unixtime: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[      9132,      14138,          5, 1477785600],\n",
       "       [     10612,      14138,          5, 1467244800],\n",
       "       [       257,      14138,          1, 1454716800],\n",
       "       ...,\n",
       "       [      9051,      20315,          5, 1530144000],\n",
       "       [      3412,      20315,          5, 1527465600],\n",
       "       [      9805,      20315,          5, 1527206400]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_dims=0\n",
    "for i in range(data.shape[1] - 2):  # do not affect to timestamp and rating // origin::: for i in range(data.shape[1] - 1)\n",
    "    # MAKE IT START BY 0\n",
    "    data[:, i] -= np.min(data[:, i])\n",
    "    # RE-INDEX\n",
    "    data[:, i] += add_dims\n",
    "    add_dims = np.max(data[:, i]) + 1\n",
    "dims = np.max(data, axis=0) + 1\n",
    "print(\"Dim of users: {}\\nDim of items: {}\\nDims of unixtime: {}\".format(dims[0], dims[1], dims[2]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137364"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20315"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14137, 20315, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([     14138,      20316,          6, 1538006401])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [0,1,2]\n",
    "print([max(data[:,col]) for col in cols])\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting train/test and removing timestamp...: 100%|██████████| 14138/14138 [00:05<00:00, 2589.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248,     5],\n",
       "       [    0, 19249,     5],\n",
       "       [    0, 14823,     4],\n",
       "       ...,\n",
       "       [14137, 14159,     5],\n",
       "       [14137, 18245,     5],\n",
       "       [14137, 18904,     5]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x = split_train_test(data, dims[0], strategy)\n",
    "train_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14138, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249,     5],\n",
       "       [    1, 18015,     5],\n",
       "       [    2, 14196,     4],\n",
       "       ...,\n",
       "       [14135, 19938,     5],\n",
       "       [14136, 20214,     2],\n",
       "       [14137, 15542,     3]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dims: [14138 20316]\n",
      "New train_x:\n",
      " [[    0 19248     5]\n",
      " [    0 19249     5]\n",
      " [    0 14823     4]\n",
      " ...\n",
      " [14137 14159     5]\n",
      " [14137 18245     5]\n",
      " [14137 18904     5]]\n"
     ]
    }
   ],
   "source": [
    "# train_x = train_x[:, :2] # quitar el rating\n",
    "dims = dims[:2]\n",
    "print(\"New dims:\",dims)\n",
    "print(\"New train_x:\\n\",train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adj_mx(n_feat:int, data:np.ndarray) -> sp.dok_matrix :\n",
    "    train_mat = sp.dok_matrix((n_feat, n_feat), dtype=np.float32)\n",
    "    for x in tqdm(data, desc=f\"BUILDING ADJACENCY MATRIX...\"):\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "        train_mat[x[1], x[0]] = 1.0\n",
    "        # IDEA: We treat features that are not user or item differently because we do not consider\n",
    "        #  interactions between contexts\n",
    "        if data.shape[1] > 2:\n",
    "            for idx in range(len(x[2:])):\n",
    "                train_mat[x[0], x[2 + idx]] = 1.0\n",
    "                train_mat[x[1], x[2 + idx]] = 1.0\n",
    "                train_mat[x[2 + idx], x[0]] = 1.0\n",
    "                train_mat[x[2 + idx], x[1]] = 1.0\n",
    "    return train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ng_sample(data: np.ndarray, dims: list, num_ng:int=4) -> Tuple[np.ndarray, sp.dok_matrix]:\n",
    "    rating_mat = build_adj_mx(dims[-1], data)\n",
    "    interactions = []\n",
    "    min_item, max_item = dims[0], dims[1]\n",
    "    for num, x in tqdm(enumerate(data), desc='perform negative sampling...'):\n",
    "        interactions.append(np.append(x, 1))\n",
    "        for t in range(num_ng):\n",
    "            j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
    "            while (x[0], j) in rating_mat or j == int(x[1]):\n",
    "                j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            interactions.append(np.concatenate([[x[0], j], x[2:], [0]]))\n",
    "    return np.vstack(interactions), rating_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19248</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>19249</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>14823</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>15455</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>15692</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123221</th>\n",
       "      <td>14137</td>\n",
       "      <td>17250</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123222</th>\n",
       "      <td>14137</td>\n",
       "      <td>18304</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123223</th>\n",
       "      <td>14137</td>\n",
       "      <td>14159</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123224</th>\n",
       "      <td>14137</td>\n",
       "      <td>18245</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123225</th>\n",
       "      <td>14137</td>\n",
       "      <td>18904</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123226 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID   asin  rank\n",
       "0                0  19248     5\n",
       "1                0  19249     5\n",
       "2                0  14823     4\n",
       "3                0  15455     5\n",
       "4                0  15692     5\n",
       "...            ...    ...   ...\n",
       "123221       14137  17250     5\n",
       "123222       14137  18304     5\n",
       "123223       14137  14159     5\n",
       "123224       14137  18245     5\n",
       "123225       14137  18904     5\n",
       "\n",
       "[123226 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_x, columns=[ \"reviewerID\",\"asin\",\"rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "items_sorted = pd.DataFrame(train_x[:,:2], columns=[ \"reviewerID\",\"asin\"]).groupby(\"asin\").count().sort_values(by=\"reviewerID\",ascending=False).reset_index()\n",
    "items_sorted.asin = items_sorted.asin.astype(str)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "def plot(df, title_name, figure_name, fig_size):\n",
    "    plt.figure(figsize=(fig_size[0],fig_size[1]), dpi=100)\n",
    "    plt.xticks(rotation=90)\n",
    "    sns_plot = sns.barplot(data =df, x=\"asin\", y=\"reviewerID\")\n",
    "    sns_plot.set(xlabel='Product ID', ylabel='count',title=title_name)\n",
    "    \n",
    "    sns_plot.figure.savefig(figure_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6178, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\brend\\AppData\\Local\\Temp\\ipykernel_4104\\1994285982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_sorted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_name\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"Items sorted\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigure_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"top1000_items\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_sorted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_name\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"Items sorted\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigure_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"top10_items\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_sorted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_name\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"Items sorted\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigure_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_items_sorted\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\brend\\AppData\\Local\\Temp\\ipykernel_4104\\1974830217.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(df, title_name, figure_name, fig_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfig_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msns_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"asin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"reviewerID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0msns_plot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Product ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtitle_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\seaborn\\_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m             )\n\u001b[0;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[1;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3188\u001b[0m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3190\u001b[1;33m     \u001b[0mplotter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3191\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, ax, bar_kws)\u001b[0m\n\u001b[0;32m   1637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbar_kws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m         \u001b[1;34m\"\"\"Make the plot.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbar_kws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"h\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mdraw_bars\u001b[1;34m(self, ax, kws)\u001b[0m\n\u001b[0;32m   1603\u001b[0m             \u001b[1;31m# Draw the bars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m             barfunc(barpos, self.statistic, self.width,\n\u001b[1;32m-> 1605\u001b[1;33m                     color=self.colors, align=\"center\", **kws)\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[1;31m# Draw the confidence intervals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mbar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2492\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0morientation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'horizontal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2493\u001b[0m                 \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msticky_edges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2494\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2495\u001b[0m             \u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_patch\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m   2030\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2031\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2032\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2033\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2034\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[1;34m(self, path, transform)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m                 self.clipbox = TransformedBbox(Bbox.unit(),\n\u001b[1;32m--> 754\u001b[1;33m                                                path.get_transform())\n\u001b[0m\u001b[0;32m    755\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clippath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;34m\"\"\"Return the `~.transforms.Transform` applied to the `Patch`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36m_update_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[0mrot_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m         \u001b[0mrot_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_deg_around\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBboxTransformTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrot_trans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mrotate_deg_around\u001b[1;34m(self, x, y, degrees)\u001b[0m\n\u001b[0;32m   1965\u001b[0m         \u001b[1;31m# Cast to float to avoid wraparound issues with uint8's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1967\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_deg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1969\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mrotate_deg\u001b[1;34m(self, degrees)\u001b[0m\n\u001b[0;32m   1943\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1944\u001b[0m         \"\"\"\n\u001b[1;32m-> 1945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1947\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrotate_around\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(self, theta)\u001b[0m\n\u001b[0;32m   1931\u001b[0m         rotate_mtx = np.array([[a, -b, 0.0], [b, a, 0.0], [0.0, 0.0, 1.0]],\n\u001b[0;32m   1932\u001b[0m                               float)\n\u001b[1;32m-> 1933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotate_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1934\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1935\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(items_sorted.shape)\n",
    "# plot(items_sorted.head(1000), title_name= \"Items sorted\", figure_name=\"top1000_items\", fig_size=[100,20])\n",
    "# plot(items_sorted.head(10), title_name= \"Items sorted\", figure_name=\"top10_items\", fig_size=[15,10])\n",
    "# plot(items_sorted, title_name= \"Items sorted\", figure_name=\"train_items_sorted\", fig_size=[500,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'items_sorted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\brend\\AppData\\Local\\Temp\\ipykernel_14680\\2106879318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mitems_sorted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpopularity_recommendations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_popularity_recommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpopularity_recommendations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'items_sorted' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def get_popularity_recommendations(items_sorted):\n",
    "  return items_sorted.asin.to_numpy()\n",
    "\n",
    "popularity_recommendations = get_popularity_recommendations(items_sorted)\n",
    "popularity_recommendations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18904, 18524, 14526, 18964, 19679, 19680, 20108, 18541, 18584,\n",
       "       14360])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from scipy.stats import rankdata\n",
    "\n",
    "# def get_popularity_recommendations(train_x, dims):\n",
    "\n",
    "#   train_x_rank = np.insert(np.array(train_x), train_x.shape[1], rankdata(train_x[:,-1]), 1)\n",
    "#   pop_rec = train_x[train_x_rank[:,-1].argsort()[::-1]][:,1]- dims[0]\n",
    "#   pop_rec = [i for n, i in enumerate(pop_rec) if i not in pop_rec[:n]] \n",
    "  \n",
    "#   return np.hstack(pop_rec)\n",
    "\n",
    "# popularity_recommendations = get_popularity_recommendations(train_x, dims)\n",
    "# popularity_recommendations[:10] + dims[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18904, 18524, 14526, 18964, 19679, 19680, 20108, 18541, 18584,\n",
       "       14360])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_rec = popularity_recommendations[:10] +dims[0]\n",
    "top10_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING ADJACENCY MATRIX...: 100%|██████████| 123226/123226 [00:10<00:00, 11952.54it/s]\n",
      "perform negative sampling...: 123226it [00:06, 19093.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions matrix:\n",
      " [14138 20316]\n",
      "\n",
      "Rating matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<20316x20316 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 347600 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, rating_mat = ng_sample(train_x, dims)\n",
    "print(\"Dimensions matrix:\\n\",dims)\n",
    "print(\"\\nRating matrix:\")\n",
    "rating_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6178"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims[-1]-dims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008421769667914019\n",
      "0.9991578230332085\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Evaristo\n",
    "#### number of ones\n",
    "print(np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "### number of zeros\n",
    "print(1 - np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "\n",
    "# ## Brenda\n",
    "# #### Who sparse is the matrix??\n",
    "# print(1 - rating_mat.shape[0] / rating_mat.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[:,[0,1,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248,     1],\n",
       "       [    0, 14547,     0],\n",
       "       [    0, 20139,     0],\n",
       "       [    0, 19680,     0],\n",
       "       [    0, 18083,     0],\n",
       "       [    0, 19249,     1],\n",
       "       [    0, 15410,     0],\n",
       "       [    0, 20082,     0],\n",
       "       [    0, 16041,     0],\n",
       "       [    0, 19078,     0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointData(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 dims: list) -> None:\n",
    "        \"\"\"\n",
    "        Dataset formatter adapted point-wise algorithms\n",
    "        Parameters\n",
    "        \"\"\"\n",
    "        super(PointData, self).__init__()\n",
    "        self.interactions = data\n",
    "        self.dims = dims\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.interactions)\n",
    "        \n",
    "    def __getitem__(self, \n",
    "                    index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return the pairs user-item and the target.\n",
    "        \"\"\"\n",
    "        return self.interactions[index][:-1], self.interactions[index][-1]\n",
    "\n",
    "train_dataset = PointData(train_x, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0, 19248]), 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the test set for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249,     5],\n",
       "       [    1, 18015,     5],\n",
       "       [    2, 14196,     4],\n",
       "       ...,\n",
       "       [14135, 19938,     5],\n",
       "       [14136, 20214,     2],\n",
       "       [14137, 15542,     3]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20316, 20316)\n",
      "rating_mat contains log2(rating_mat.shape[0]) = 15 bits\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(rating_mat.shape)\n",
    "bits = math.ceil(math.log(rating_mat.shape[0],2))\n",
    "print(\"rating_mat contains log2(rating_mat.shape[0]) = {} bits\".format(bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_test_no_interactions(train_x: np.ndarray, test_x: np.ndarray, dims_usuarios_productos: Tuple[int, int],  num_samples: int) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Esta funcion se encarga de crear de manera eficiente un dataset que contenga las interacciones usuario-producto en test que no se hayan producido.\n",
    "    \n",
    "#     Argumentos:\n",
    "#         train_x (np.ndarray): matriz de entrenamiento con las interacciones usuario-producto previas\n",
    "#         test_x (np.ndarray): matriz de prueba con las interacciones usuario-producto previas\n",
    "#         dims_usuarios_productos (Tuple[int, int]): rango de productos y usuarios disponibles\n",
    "    \n",
    "#     Retorno:\n",
    "#         np.ndarray: una matriz con todas las interacciones usuario-producto en test que no se hayan producido\n",
    "#     \"\"\"\n",
    "#     from tqdm import tqdm\n",
    "#     import random\n",
    "    \n",
    "#     # Identificamos los usuarios presentes en la prueba\n",
    "#     usuarios_test = np.unique(test_x[:, 0])\n",
    "#     # Identificamos el rango de productos disponibles\n",
    "#     total_productos = range(dims_usuarios_productos[0]-1, dims_usuarios_productos[1])\n",
    "    \n",
    "#     # Recorremos cada usuario presente en la prueba\n",
    "#     for usuario in tqdm(usuarios_test):\n",
    "#         # Identificamos los productos en los que el usuario ha interactuado previamente en entrenamiento\n",
    "#         productos_train = np.unique(train_x[train_x[:, 0] == usuario][:, 1])\n",
    "#         # Seleccionamos al azar 199 productos con los que el usuario no ha interactuado previamente\n",
    "#         productos_a_machear = random.choices(list(set(total_productos) - set(productos_train)), k=num_samples)\n",
    "#         # Creamos una lista de interacciones usuario-producto para este usuario\n",
    "#         # lista_por_usuario = [[usuario, x] for x in productos_a_machear]\n",
    "#         # We must avoid using for loops!\n",
    "#         lista_por_usuario = np.vstack([(np.ones(len(productos_a_machear))*usuario).astype(int), productos_a_machear]).T\n",
    "        \n",
    "#         # Si es el primer usuario, inicializamos una matriz con sus interacciones\n",
    "#         if usuario == 0:\n",
    "#             zero_positions = lista_por_usuario\n",
    "#         # Si no es el primer usuario, concatenamos sus interacciones a la matriz existente\n",
    "#         else:\n",
    "#             zero_positions = np.concatenate((zero_positions, lista_por_usuario), axis=0)\n",
    "            \n",
    "#     return zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ng_test(rating_mat):\n",
    "    # save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data  \"+\"#\"*4)\n",
    "    # return np.asarray(np.where(rating_mat.A==0)).T\n",
    "    zero_true_matrix = np.where(rating_mat.A==0)\n",
    "    save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data separated by rows  \"+\"#\"*4)\n",
    "    return np.asarray([zero_true_matrix[0],zero_true_matrix[1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412392256, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0],\n",
       "       [    0,     1],\n",
       "       [    0,     2],\n",
       "       ...,\n",
       "       [20315, 20313],\n",
       "       [20315, 20314],\n",
       "       [20315, 20315]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero_positions = np.asarray(np.where(rating_mat.A==0)).T\n",
    "zero_positions = ng_test(rating_mat)\n",
    "print(save_data_configuration(str(zero_positions.shape)+\"\\n\"))\n",
    "zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           i\n",
      "u           \n",
      "0      14138\n",
      "0      14139\n",
      "0      14140\n",
      "0      14141\n",
      "0      14142\n",
      "...      ...\n",
      "20315  20311\n",
      "20315  20312\n",
      "20315  20313\n",
      "20315  20314\n",
      "20315  20315\n",
      "\n",
      "[125370541 rows x 1 columns]\n",
      "(125370541, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20316/20316 [02:27<00:00, 138.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def items_to_compute(zero_positions, dims):\n",
    "\n",
    "    # LAB\n",
    "    # items2compute = []\n",
    "    # for user in trange(dims[0]):\n",
    "    #     aux = zero_positions[zero_positions[:, 0] == user][:, 1]\n",
    "    #     items2compute.append(aux[aux >= dims[0]])\n",
    "    # items2compute[0]\n",
    "\n",
    "    # # ANTONIO\n",
    "    # mask = zero_positions[:, 1] >= dims[0]\n",
    "    # # Seleccionar los valores de la columna 1 que cumplen la condición\n",
    "    # zero_positions = zero_positions[mask]\n",
    "    # # Supongamos que el array se llama 'arr'\n",
    "    # usuarios = zero_positions[:, 0]\n",
    "    # lista_longitud_zeros = np.bincount(usuarios, minlength=len(set(usuarios)))\n",
    "    # # Crear lista de sublistas vacías de acuerdo a la lista de longitudes\n",
    "    # list_of_lists = [list() for i in range(len(lista_longitud_zeros))]\n",
    "    # for i, length in enumerate(lista_longitud_zeros):\n",
    "    #     # Agregar ceros a cada sublista\n",
    "    #     list_of_lists[i] = [0]*length\n",
    "    # # Convertir lista de sublistas a un array de NumPy\n",
    "    # items2compute = np.array([np.array(x) for x in list_of_lists])\n",
    "    # for user in trange(dims[0]):\n",
    "    #     aux = zero_positions[zero_positions[:, 0] == user][:, 1]\n",
    "    #     # Asignar los elementos necesarios a cada usuario\n",
    "    #     items2compute[user] = aux.copy()\n",
    "    #     # 3%|▎         | 421/14138 [03:01<1:38:46,  2.31it/s]\n",
    "\n",
    "    # JOAN\n",
    "    # items2compute = []\n",
    "    # items_zero_per_user = []\n",
    "    # for user in trange(dims[0]):\n",
    "    #     aux1 = rating_mat[user, (dims[0]+1):]        \n",
    "    #     items_zero_per_user = np.where(aux1.A==0)  # devuelve los indices\n",
    "    #     aux = items_zero_per_user[:] + (dims[0]+1) # le sumamos la dimension para ubicar correctamente la columna\n",
    "    #     aux = aux[1]\n",
    "    #     # lista items del user en train del mismo usuario que tengan rating 0 ó 1\n",
    "    #     items_train_user = train_x[train_x[:,0]==user][:,1]\n",
    "    #     # los retiramos del test (los 1 como no existen no se retiran, solo eliminan los 0)\n",
    "    #     aux = list(set(aux) - set(items_train_user))\n",
    "    #     items2compute.append(sorted(aux)) # añadimos los valores de item\n",
    "\n",
    "    # BRENDA\n",
    "    zp_df = pd.DataFrame(zero_positions[zero_positions[:, 1] >= dims[0]], columns=['u','i'])\n",
    "    zp_df_index_u = zp_df.set_index(['u']) # 55 seg\n",
    "    users = np.unique(zero_positions[:,0]) # 50 seg\n",
    "    print(zp_df_index_u)\n",
    "    print(zp_df_index_u.shape)\n",
    "\n",
    "    items2compute = list(list() for _ in users)\n",
    "    for i in trange(len(users)):\n",
    "        items2compute[i] = np.hstack(zp_df_index_u.loc[i].to_numpy())\n",
    "    # 100%|██████████| 20316/20316 [02:44<00:00, 123.50it/s]\n",
    "\n",
    "    return items2compute\n",
    "\n",
    "items2compute = items_to_compute(zero_positions, dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249,     5],\n",
       "       [    1, 18015,     5],\n",
       "       [    2, 14196,     4],\n",
       "       ...,\n",
       "       [14135, 19938,     5],\n",
       "       [14136, 20214,     2],\n",
       "       [14137, 15542,     3]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING TEST SET...: 14138it [01:52, 125.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249],\n",
       "       [    0, 14138],\n",
       "       [    0, 14139],\n",
       "       ...,\n",
       "       [    0, 20313],\n",
       "       [    0, 20314],\n",
       "       [    0, 20315]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_test_set(itemsnoninteracted:list, gt_test_interactions: np.ndarray) -> list:\n",
    "    #max_users, max_items = dims # number users (943), number items (2625)\n",
    "    test_set = []\n",
    "    for pair, negatives in tqdm(zip(gt_test_interactions, itemsnoninteracted), desc=\"BUILDING TEST SET...\"):\n",
    "        # APPEND TEST SETS FOR SINGLE USER\n",
    "        negatives = np.delete(negatives, np.where(negatives == pair[1]))\n",
    "        single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
    "        single_user_test_set[:, 1][1:] = negatives\n",
    "        test_set.append(single_user_test_set.copy())\n",
    "    return test_set\n",
    "\n",
    "test_x = build_test_set(items2compute, test_x[:,:2])\n",
    "test_x[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda train_dataset y test_x\n",
    "import pickle\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "mod_path = execution_path / Path(\"4_Modelling/mod_baseline\")\n",
    "os.makedirs(mod_path, exist_ok=True)\n",
    "timestamp = today.strftime(\"%dday%mmon%Yyear\")\n",
    "\n",
    "# Train\n",
    "with open(mod_path / f\"MOD_baseline_train_{timestamp}.pkl\", 'wb') as handle:\n",
    "    pickle.dump(train_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Test\n",
    "with open(mod_path / f\"MOD_baseline_test_{timestamp}.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_x, f)\n",
    "\n",
    "with open(mod_path / f\"MOD_baseline_popRec_{timestamp}.pkl\", 'wb') as f:\n",
    "    pickle.dump(popularity_recommendations, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Guarda train_dataset y test_x\n",
    "# import pickle\n",
    "# from datetime import date\n",
    "# import os\n",
    "\n",
    "# mod_path = execution_path / Path(\"4_Modelling/mod_baseline\")\n",
    "# assert os.path.exists(mod_path), f\"The following path does not exist: \\n{mod_path}\"\n",
    "# timestamp = \"{d}day{m}mon{y}year\".format(d=\"04\", m=\"03\", y=\"2023\")\n",
    "\n",
    "# # Train\n",
    "# with open(mod_path / f\"MOD_baseline_train_{timestamp}.pkl\", 'rb') as handle:\n",
    "#     train_dataset = pickle.load(handle)\n",
    "\n",
    "# # Test\n",
    "# with open(mod_path / f\"MOD_baseline_test_{timestamp}.pkl\", 'rb') as f:\n",
    "#     test_x = pickle.load(handle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Factorization Machines model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM_operation(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 reduce_sum: bool=True) -> None:\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        # square_of_sum = np.sum(x, dim=1) ** 2 # ...\n",
    "        # sum_of_square = np.sum(x ** 2, dim=1) # ...\n",
    "        \n",
    "        square_of_sum = torch.pow(torch.sum(x, dim=1),2)\n",
    "        sum_of_square = torch.sum(torch.pow(x,2), dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 field_dims: list,\n",
    "                 embed_dim: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(len(field_dims), 1)\n",
    "        self.embedding = torch.nn.Embedding(field_dims[-1], embed_dim)\n",
    "        self.fm = FM_operation(reduce_sum=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, interaction_pairs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        out = self.linear(interaction_pairs.float()) + self.fm(self.embedding(interaction_pairs))\n",
    "        return out.squeeze(1)\n",
    "        \n",
    "    def predict(self, \n",
    "                interactions: np.ndarray,\n",
    "                device: torch.device) -> torch.Tensor:\n",
    "        # return the score, inputs are numpy arrays, outputs are tensors\n",
    "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device) #, dtype=torch.long)\n",
    "        output_scores = self.forward(test_interactions)\n",
    "        return output_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    criterion: torch.nn.functional,\n",
    "                    device: torch.device) -> float:\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "\n",
    "    for i, (interactions, targets) in enumerate(data_loader):\n",
    "        interactions = interactions.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        predictions = model(interactions[:,:2])\n",
    "    \n",
    "        loss = criterion(predictions, targets.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    return mean(total_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getHitRatio(recommend_list: list,\n",
    "                gt_item: int) -> bool:\n",
    "    if gt_item in recommend_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getNDCG(recommend_list: list,\n",
    "            gt_item: int) -> float:\n",
    "    idx = np.where(recommend_list == gt_item)[0]\n",
    "    if len(idx) > 0:\n",
    "        return math.log(2)/math.log(idx+2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module,\n",
    "         test_x: np.ndarray,\n",
    "         device: torch.device,\n",
    "         topk: int=10) -> Tuple[float, float]:\n",
    "    # Test the HR and NDCG for the model @topK\n",
    "    model.eval()\n",
    "\n",
    "    user_recommend_list = np.zeros(len(test_x)).tolist()\n",
    "    index = 0\n",
    "\n",
    "    HR, NDCG = [], []\n",
    "    for user_test in test_x:\n",
    "        gt_item = user_test[0][1]\n",
    "        predictions = model.predict(user_test, device)\n",
    "        _, indices = torch.topk(predictions, topk)\n",
    "        recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
    "        user_recommend_list[index] = recommend_list.tolist().copy()\n",
    "        index +=1\n",
    "\n",
    "        HR.append(getHitRatio(recommend_list, gt_item))\n",
    "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
    "        \n",
    "    coverage = len(set(np.hstack(user_recommend_list))) / (dims[1]-dims[0]) *100\n",
    "    \n",
    "    return mean(HR), mean(NDCG), user_recommend_list, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pop(model: torch.nn.Module,\n",
    "         test_x: np.ndarray,\n",
    "         device: torch.device,\n",
    "         topk: int=10) -> Tuple[float, float]:\n",
    "    # Test the HR and NDCG for the model @topK\n",
    "    model.eval()\n",
    "\n",
    "    user_recommend_list = np.zeros(len(test_x)).tolist()\n",
    "    index = 0\n",
    "\n",
    "    HR, NDCG = [], []\n",
    "    for user_test in test_x:\n",
    "        gt_item = user_test[0][1]\n",
    "        predictions = model.predict(user_test, device)\n",
    "        # print(predictions)\n",
    "        # _, indices = torch.topk(predictions, topk)\n",
    "        # print(indices)\n",
    "        # print(user_test)\n",
    "        recommend_list = predictions[:topk]\n",
    "        # user_test[indices.cpu().detach().numpy()][:, 1]\n",
    "        # print(recommend_list)\n",
    "        user_recommend_list[index] = np.hstack(recommend_list.tolist().copy())\n",
    "        index +=1\n",
    "\n",
    "        HR.append(getHitRatio(recommend_list, gt_item))\n",
    "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
    "        \n",
    "    coverage = len(set(np.hstack(user_recommend_list))) / (dims[1]-dims[0]) *100\n",
    "    \n",
    "    return mean(HR), mean(NDCG), user_recommend_list, coverage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE\n",
    "## Defining the model, the loss and the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = train_dataset.dims\n",
    "model = FactorizationMachineModel(dims, hparams['hidden_size']).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=hparams['learning_rate'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: list) -> None:\n",
    "        super(RandomModel, self).__init__()\n",
    "        \"\"\"\n",
    "        Simple random based recommender system\n",
    "        \"\"\"\n",
    "        self.all_items = list(range(dims[0], dims[1]))\n",
    "\n",
    "    def forward(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def predict(self,\n",
    "                interactions: np.ndarray,\n",
    "                device=None) -> torch.Tensor:\n",
    "        return torch.FloatTensor(random.sample(self.all_items, len(interactions)))\n",
    "\n",
    "rnd_model = RandomModel(dims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popularity-Based Recommender System\n",
    "class PopularityBasedModel(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  https://github.com/LaxmiChaudhary/Amzon-Product-Recommendation/blob/master/Recommendation%20System.ipynb\n",
    "  The Popularity-based recommender system is a non-personalised recommender system and these are based on frequecy counts, which may be not suitable to the user.\n",
    "  We can see the differance above for the user id 4, 6 & 8, The Popularity based model has recommended the same set of 5 products to both.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "                 popularity_recommendations) -> None:\n",
    "        super(PopularityBasedModel, self).__init__()\n",
    "        \"\"\"\n",
    "        Simple random based recommender system\n",
    "        \"\"\"\n",
    "        self.popularity_recommendations = popularity_recommendations\n",
    "  \n",
    "  def predict(self,\n",
    "              interactions: np.ndarray,\n",
    "              device=None) -> torch.Tensor:\n",
    "      return torch.IntTensor(self.popularity_recommendations)\n",
    "\n",
    "pop_model = PopularityBasedModel(popularity_recommendations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCF evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NCF(torch.nn.Module): #  with FactorizationMachine\n",
    "#     def __init__(self, \n",
    "#                  field_dims: list,\n",
    "#                  embed_dim: float) -> None:\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.embedding = torch.nn.Embedding(field_dims[-1], embed_dim)\n",
    "#         self.model_MF = FactorizationMachineModel(field_dims, embed_dim)\n",
    "#         self.fm = FM_operation()\n",
    "#         self.mlp = torch.nn.Sequential(torch.nn.Linear(len(field_dims), 1)\n",
    "\n",
    "\n",
    "#         )\n",
    "#         sequential = [[64, 16], # (8, )\n",
    "#               [16, 32],\n",
    "#               [32, 8],\n",
    "#               [8, embed_dim]] \n",
    "#         self.last_fc = torch.nn.Linear(9, 1)\n",
    "#         self.mlp = torch.nn.Sequential(torch.nn.Linear(64, 32),\n",
    "#                           torch.nn.ReLU(),\n",
    "#                           torch.nn.Linear(32, 16),\n",
    "#                           torch.nn.ReLU(),\n",
    "#                           torch.nn.Linear(16, 8),\n",
    "#                         #   torch.nn.ReLU(),\n",
    "#                         #   torch.nn.Linear(sequential[3][0], sequential[3][1]),\n",
    "#                           torch.nn.ReLU())\n",
    "#         self.linear = torch.nn.Linear(len(field_dims), 1)\n",
    "#     def FactorizationMachine(self, interaction_pairs): # SI FUNCIONA, IMPLEMENTAR FactrizationMachineModel\n",
    "#         return self.linear(interaction_pairs.float()) + self.fm(self.embedding(interaction_pairs))\n",
    "\n",
    "#     def forward(self, interaction_pairs: torch.Tensor) -> torch.Tensor:\n",
    "#         user = torch.from_numpy(interaction_pairs[:,0]).to(dtype=torch.long, device=device)\n",
    "#         items = torch.from_numpy(interaction_pairs[:,1]).to(dtype=torch.long, device=device)\n",
    "#         inter_tensor = torch.from_numpy(interaction_pairs).to(dtype=torch.long, device=device)\n",
    "\n",
    "#         item_embedding_mlp = self.embedding(items)\n",
    "#         user_embedding_mlp = self.embedding(user)\n",
    "#         # print(interaction_pairs, interaction_pairs.shape)\n",
    "\n",
    "\n",
    "#         mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
    "#         # mf_vector = torch.mul(user_embedding_mf, item_embedding_mf)     # No usamos MatrixFactorization\n",
    "#         # print(\"mf_vector.shape\", mf_vector.shape)                       # No usamos MatrixFactorization\n",
    "# #        print(\"mlp_vector.shape\", mlp_vector.shape)\n",
    "#         fm_vector = self.FactorizationMachine(inter_tensor)\n",
    "# #        print(\"fm_vector.shape\", fm_vector.shape)\n",
    "\n",
    "#         mlp_vector = self.mlp(mlp_vector)\n",
    "# #       print(\"mlp_vector.shape after MLP sequence\", mlp_vector.shape)\n",
    "# #        print(\"embedd dim: \", self.embed_dim)\n",
    "#         concatenation= torch.cat([mlp_vector, fm_vector], dim=-1)\n",
    "# #        print(\"concatenation.shape\", concatenation.shape)\n",
    "#         concatenation = self.last_fc(concatenation)\n",
    "# #        print(\"Concatenation after Linear(9,1).shape\", concatenation.shape)\n",
    "       \n",
    "        \n",
    "#         return concatenation.squeeze()\n",
    "\n",
    "#     def predict(self,\n",
    "#                 interactions: np.ndarray,\n",
    "#                 device: torch.device) -> torch.Tensor:\n",
    "        \n",
    "#         output_scores = self.forward(interactions)\n",
    "#         return output_scores\n",
    "\n",
    "# ncf_model = NCF(dims, hparams['hidden_size']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 field_dims: list,\n",
    "                 embed_dim: float) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = torch.nn.Embedding(field_dims[-1], embed_dim)\n",
    "        sequential = [[self.embed_dim*2, 32], [32, 16], [16, 8]]\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(sequential[0][0], sequential[0][1]), #  (64, 32)\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(sequential[1][0], sequential[1][1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(sequential[2][0], sequential[2][1]),\n",
    "                        torch.nn.ReLU())\n",
    "        self.last_fc = torch.nn.Linear(sequential[2][1]+self.embed_dim, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, interaction_pairs: np.ndarray) -> torch.Tensor:\n",
    "        user_embedding_mf = self.embedding(interaction_pairs[:,0])\n",
    "        item_embedding_mf = self.embedding(interaction_pairs[:,1])\n",
    "\n",
    "        # MLP vector reshaped to 2 dimensions\n",
    "        mlp_vector = self.embedding(interaction_pairs)  # (6142, 2, 32)\n",
    "        mlp_vector = mlp_vector.reshape(-1, self.embed_dim*2) # (6142, 64)\n",
    "\n",
    "        # MatrixFactorization vector\n",
    "        mf_vector = torch.mul(user_embedding_mf, item_embedding_mf) # ( _ , 32)\n",
    "\n",
    "        # MLP thought layers\n",
    "        mlp_vector = self.mlp(mlp_vector) # ( _ , sequential[2][1]) = ( _ , 8)\n",
    "\n",
    "        # NCF: concat MLP_output and MF_output\n",
    "        # NCF: Last fully connected layer of size (40x1) --> (8: mlp output + 32: mf output => 40)\n",
    "        # NCF: Activation function ReLU\n",
    "        concatenation = torch.cat([mlp_vector, mf_vector], dim=-1) # (_, 40)\n",
    "        concatenation = self.last_fc(concatenation)\n",
    "        # concatenation = self.relu(concatenation)\n",
    "        return concatenation.squeeze()\n",
    "\n",
    "    def predict(self,\n",
    "                interactions: np.ndarray,\n",
    "                device: torch.device) -> torch.Tensor:\n",
    "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device)\n",
    "        output_scores = self.forward(test_interactions)\n",
    "        return output_scores\n",
    "\n",
    "ncf_model = NCF(dims, hparams['hidden_size']).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO EPOCHS NOW\n",
    "from datetime import datetime\n",
    "import time\n",
    "save_data_configuration(datetime.now().strftime(\"%d-%b-%Y  %H:%M\"))\n",
    "time_start = time.time()\n",
    "topk = 10\n",
    "\n",
    "fm = np.zeros([hparams['num_epochs'],3])\n",
    "rnd = np.zeros([hparams['num_epochs'],3])\n",
    "pop = np.zeros([hparams['num_epochs'],3])\n",
    "ncf = np.zeros([hparams['num_epochs'],3])\n",
    "\n",
    "for epoch_i in range(hparams['num_epochs']):\n",
    "    #data_loader.dataset.negative_sampling()\n",
    "    train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
    "\n",
    "    hr, ndcg, recommend_list_fm, cov_fm = test(model, test_x, device, topk=topk)\n",
    "    print(save_data_configuration(f'MODEL: FACTORIZATION MACHINE'))\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    fm[epoch_i] = [hr, ndcg, cov_fm]\n",
    "    tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
    "    tb_fm.add_scalar(f'eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_fm.add_scalar(f'eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "    hr, ndcg, recommend_list_rnd, cov_rnd = test(rnd_model, test_x, device, topk=topk)\n",
    "    print(save_data_configuration(f'MODEL: RANDOM'))\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    rnd[epoch_i] = [hr, ndcg, cov_rnd]\n",
    "    tb_rnd.add_scalar(f'eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_rnd.add_scalar(f'eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "    hr, ndcg, recommend_list_pop, cov_pop = test_pop(pop_model, test_x, device, topk=topk)\n",
    "    print(save_data_configuration(f'MODEL: POPULARITY-BASED'))\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    pop[epoch_i] = [hr, ndcg, cov_pop]\n",
    "    tb_pop.add_scalar(f'eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_pop.add_scalar(f'eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "    hr, ndcg, recommend_list_ncf, cov_ncf = test(ncf_model, test_x, device, topk=topk)\n",
    "    print(save_data_configuration(f'MODEL: NEURAL COLLABORATIVE FILTERING'))\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    ncf[epoch_i] = [hr, ndcg, cov_ncf]\n",
    "    tb_pop.add_scalar(f'eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_pop.add_scalar(f'eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "    save_data_configuration(\"_\"*65)\n",
    " \n",
    "save_data_configuration(f\"# Training duration: {(time.time()-time_start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO EPOCHS NOW\n",
    "# from datetime import datetime\n",
    "# save_data_configuration(datetime.now().strftime(\"%d-%b-%Y  %H:%M\"))\n",
    "# time_start = time.time()\n",
    "# topk = 10\n",
    "# for epoch_i in range(hparams['num_epochs']):\n",
    "#     #data_loader.dataset.negative_sampling()\n",
    "#     train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
    "#     hr, ndcg, recommend_list_fm, cov_fm = test(model, test_x, device, topk=topk)\n",
    "    \n",
    "#     print(save_data_configuration(f'MODEL: FACTORIZATION MACHINE'))\n",
    "#     print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "#     print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    " \n",
    "#     tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
    "#     tb_fm.add_scalar(f'eval/HR@{topk}', hr, epoch_i)\n",
    "#     tb_fm.add_scalar(f'eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "#     hr, ndcg, recommend_list_rnd, cov_rnd = test(rnd_model, test_x, device, topk=topk)\n",
    "\n",
    "#     print(save_data_configuration(f'MODEL: RANDOM'))\n",
    "#     print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "#     print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    \n",
    "#     tb_rnd.add_scalar(f'eval/HR@{topk}', hr, epoch_i)\n",
    "#     tb_rnd.add_scalar(f'eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "#     hr, ndcg, recommend_list_pop, cov_pop = test_pop(pop_model, test_x, device, topk=topk)\n",
    "    \n",
    "#     tb_pop.add_scalar(f'eval/HR@{topk}', hr, epoch_i)\n",
    "#     tb_pop.add_scalar(f'eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "#     # save_data_configuration(\"_\"*65)\n",
    "#     print(save_data_configuration(f'MODEL: POPULARITY-BASED'))\n",
    "#     print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "#     print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "#     save_data_configuration(\"_\"*65)\n",
    " \n",
    "# save_data_configuration(f\"# Training duration: {(time.time()-time_start):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage from FM: 5.924247329232761\n",
      "Coverage from RAND: 100.0\n",
      "Coverage from POP: 0.16186468112657817\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coverage from FM: {cov_fm}\")\n",
    "print(f\"Coverage from RAND: {cov_rnd}\")\n",
    "print(f\"Coverage from POP: {cov_pop}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(os.getcwd() / Path(\"4_Modelling\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir run_tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsysproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51830aba456e748017c0629249509085e32f77443cd9009188a4b3ec34121472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
