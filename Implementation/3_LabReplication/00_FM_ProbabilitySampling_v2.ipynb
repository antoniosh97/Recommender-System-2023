{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#====================== Import de librerias =====================#\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import wget\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from tqdm import tqdm, trange\n",
    "from IPython import embed\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "current_filename = sys.argv[0].split(\"\\\\\")[-1].split(\".\")[-2]\n",
    "logfile = \"project.log\"\n",
    "old_path = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "execution_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "#logs_base_dir = execution_path / Path(\"4_Modelling/runs\")\n",
    "logs_base_dir = \"runs_random\"\n",
    "os.makedirs(f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}', exist_ok=True)\n",
    "tb_fm = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_FM/')\n",
    "tb_rnd = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_RANDOM/')\n",
    "\n",
    "def save_data_configuration(text):\n",
    "    save_data_dir = \"data_config.txt\" + current_filename  \n",
    "    path = f'{execution_path}/{\"4_Modelling\"}/{save_data_dir}'\n",
    "    with open(path, \"a\") as data_file:\n",
    "        data_file.write(text+\"\\n\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some hyper-parameters\n",
    "hparams = {\n",
    "    'batch_size':64,\n",
    "    'num_epochs':12,\n",
    "    'hidden_size': 32,\n",
    "    'learning_rate':1e-4,\n",
    "}\n",
    "\n",
    "# we select to work on GPU if it is available in the machine, otherwise\n",
    "# will run on CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Definicion de valores de configuracion ============#\n",
    "\n",
    "min_reviews, min_usuarios = [6,6]\n",
    "col_names = {\"col_id_reviewer\": \"reviewerID\",\n",
    "             \"col_id_product\": \"asin\",\n",
    "             \"col_unix_time\": \"unixReviewTime\",\n",
    "             \"col_rating\": \"overall\",\n",
    "             \"col_timestamp\": \"timestamp\",\n",
    "             \"col_year\": \"year\"}\n",
    "\n",
    "csv_filename = execution_path/Path(\"3_DataPreparation/interactions_minR{}_minU{}.csv\".format(min_reviews,min_usuarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1477785600</td>\n",
       "      <td>2016-10-30 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10612</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1467244800</td>\n",
       "      <td>2016-06-30 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1454716800</td>\n",
       "      <td>2016-02-06 01:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1434844800</td>\n",
       "      <td>2015-06-21 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2523</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1420329600</td>\n",
       "      <td>2015-01-04 01:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asin  reviewerID  overall  unixReviewTime            timestamp  year\n",
       "0     0        9132      5.0      1477785600  2016-10-30 02:00:00  1970\n",
       "1     0       10612      5.0      1467244800  2016-06-30 02:00:00  1970\n",
       "2     0         257      1.0      1454716800  2016-02-06 01:00:00  1970\n",
       "3     0        4425      5.0      1434844800  2015-06-21 02:00:00  1970\n",
       "4     0        2523      4.0      1420329600  2015-01-04 01:00:00  1970"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin               6178\n",
       "reviewerID        14138\n",
       "overall               5\n",
       "unixReviewTime     3622\n",
       "timestamp          3622\n",
       "year                  1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_configuration(str(df.nunique()))\n",
    "df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset (TLOO strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data: np.ndarray,\n",
    "                     n_users: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Split and remove timestamp\n",
    "    train_x, test_x = [], []\n",
    "    for u in trange(n_users, desc='spliting train/test and removing timestamp...'):\n",
    "        user_data = data[data[:, 0] == u]\n",
    "        sorted_data = user_data[user_data[:, -1].argsort()]\n",
    "        if len(sorted_data) == 1:\n",
    "            train_x.append(sorted_data[0][:-1])\n",
    "        else:\n",
    "            train_x.append(sorted_data[:-1][:, :-1])\n",
    "            test_x.append(sorted_data[-1][:-1])\n",
    "    return np.vstack(train_x), np.stack(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      9132,          0, 1477785600],\n",
       "       [     10612,          0, 1467244800],\n",
       "       [       257,          0, 1454716800],\n",
       "       ...,\n",
       "       [      9051,       6177, 1530144000],\n",
       "       [      3412,       6177, 1527465600],\n",
       "       [      9805,       6177, 1527206400]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[*col_names.values()][:3]].astype('int32').to_numpy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim of users: 14138\n",
      "Dim of items: 20316\n",
      "Dims of unixtime: 1538006401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[      9132,      14138, 1477785600],\n",
       "       [     10612,      14138, 1467244800],\n",
       "       [       257,      14138, 1454716800],\n",
       "       ...,\n",
       "       [      9051,      20315, 1530144000],\n",
       "       [      3412,      20315, 1527465600],\n",
       "       [      9805,      20315, 1527206400]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_dims=0\n",
    "for i in range(data.shape[1] - 1):  # do not affect to timestamp\n",
    "    # MAKE IT START BY 0\n",
    "    data[:, i] -= np.min(data[:, i])\n",
    "    # RE-INDEX\n",
    "    data[:, i] += add_dims\n",
    "    add_dims = np.max(data[:, i]) + 1\n",
    "dims = np.max(data, axis=0) + 1\n",
    "print(\"Dim of users: {}\\nDim of items: {}\\nDims of unixtime: {}\".format(dims[0], dims[1], dims[2]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting train/test and removing timestamp...: 100%|██████████| 14138/14138 [00:05<00:00, 2655.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248],\n",
       "       [    0, 19249],\n",
       "       [    0, 14823],\n",
       "       ...,\n",
       "       [14137, 14159],\n",
       "       [14137, 18245],\n",
       "       [14137, 18904]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x = split_train_test(data, dims[0])\n",
    "train_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dims: [14138 20316]\n",
      "New train_x:\n",
      " [[    0 19248]\n",
      " [    0 19249]\n",
      " [    0 14823]\n",
      " ...\n",
      " [14137 14159]\n",
      " [14137 18245]\n",
      " [14137 18904]]\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x[:, :2]\n",
    "dims = dims[:2]\n",
    "print(\"New dims:\",dims)\n",
    "print(\"New train_x:\\n\",train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adj_mx(n_feat:int, data:np.ndarray) -> sp.dok_matrix :\n",
    "    train_mat = sp.dok_matrix((n_feat, n_feat), dtype=np.float32)\n",
    "    for x in tqdm(data, desc=f\"BUILDING ADJACENCY MATRIX...\"):\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "        train_mat[x[1], x[0]] = 1.0\n",
    "        # IDEA: We treat features that are not user or item differently because we do not consider\n",
    "        #  interactions between contexts\n",
    "        if data.shape[1] > 2:\n",
    "            for idx in range(len(x[2:])):\n",
    "                train_mat[x[0], x[2 + idx]] = 1.0\n",
    "                train_mat[x[1], x[2 + idx]] = 1.0\n",
    "                train_mat[x[2 + idx], x[0]] = 1.0\n",
    "                train_mat[x[2 + idx], x[1]] = 1.0\n",
    "    return train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ng_sample(data: np.ndarray, dims: list, num_ng:int=4) -> Tuple[np.ndarray, sp.dok_matrix]:\n",
    "    rating_mat = build_adj_mx(dims[-1], data)\n",
    "    interactions = []\n",
    "    min_item, max_item = dims[0], dims[1]\n",
    "    for num, x in tqdm(enumerate(data), desc='perform negative sampling...'):\n",
    "        interactions.append(np.append(x, 1))\n",
    "        for t in range(num_ng):\n",
    "            j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
    "            while (x[0], j) in rating_mat or j == int(x[1]):\n",
    "                j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            interactions.append(np.concatenate([[x[0], j], x[2:], [0]]))\n",
    "    return np.vstack(interactions), rating_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING ADJACENCY MATRIX...: 100%|██████████| 123226/123226 [00:03<00:00, 34590.16it/s]\n",
      "perform negative sampling...: 123226it [00:06, 20100.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions matrix:\n",
      " [14138 20316]\n",
      "\n",
      "Rating matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<20316x20316 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 246452 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, rating_mat = ng_sample(train_x, dims)\n",
    "print(\"Dimensions matrix:\\n\",dims)\n",
    "print(\"\\nRating matrix:\")\n",
    "rating_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6178"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims[-1]-dims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000597112191656141\n",
      "0.9994028878083439\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Evaristo\n",
    "#### number of ones\n",
    "print(np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "### number of zeros\n",
    "print(1 - np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "\n",
    "# ## Brenda\n",
    "# #### Who sparse is the matrix??\n",
    "# print(1 - rating_mat.shape[0] / rating_mat.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248,     1],\n",
       "       [    0, 18119,     0],\n",
       "       [    0, 18743,     0],\n",
       "       [    0, 20009,     0],\n",
       "       [    0, 15350,     0],\n",
       "       [    0, 19249,     1],\n",
       "       [    0, 14495,     0],\n",
       "       [    0, 14829,     0],\n",
       "       [    0, 14609,     0],\n",
       "       [    0, 14508,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointData(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 dims: list) -> None:\n",
    "        \"\"\"\n",
    "        Dataset formatter adapted point-wise algorithms\n",
    "        Parameters\n",
    "        \"\"\"\n",
    "        super(PointData, self).__init__()\n",
    "        self.interactions = data\n",
    "        self.dims = dims\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.interactions)\n",
    "        \n",
    "    def __getitem__(self, \n",
    "                    index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return the pairs user-item and the target.\n",
    "        \"\"\"\n",
    "        return self.interactions[index][:-1], self.interactions[index][-1]\n",
    "\n",
    "train_dataset = PointData(train_x, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0, 19248]), 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the test set for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249],\n",
       "       [    1, 18015],\n",
       "       [    2, 14196],\n",
       "       ...,\n",
       "       [14135, 19938],\n",
       "       [14136, 20214],\n",
       "       [14137, 15542]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20316, 20316)\n",
      "rating_mat contains log2(rating_mat.shape[0]) = 15 bits\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(rating_mat.shape)\n",
    "bits = math.ceil(math.log(rating_mat.shape[0],2))\n",
    "print(\"rating_mat contains log2(rating_mat.shape[0]) = {} bits\".format(bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_no_interactions_stratified_users(df: pd.DataFrame,\n",
    "                                           test_x: np.ndarray,\n",
    "                                           train_x: np.ndarray, \n",
    "                                           dims_usuarios_productos: Tuple[int, int], \n",
    "                                           num_estratos: int,\n",
    "                                           num_samples: int,\n",
    "                                           le_producto) -> np.ndarray:\n",
    "\n",
    "  \"\"\"\n",
    "  Función: create_test_no_interactions_stratified\n",
    "\n",
    "  Argumentos:\n",
    "  - df: un objeto de tipo DataFrame de pandas que contiene las interacciones usuario-producto en entrenamiento.\n",
    "  - test_x: un objeto de tipo ndarray de numpy que contiene las interacciones usuario-producto de prueba.\n",
    "  - train_x: un objeto de tipo ndarray de numpy que contiene las interacciones usuario-producto en entrenamiento.\n",
    "  - dims_usuarios_productos: una tupla de enteros que contiene el número de usuarios y productos en el conjunto de datos.\n",
    "  - num_estratos: un entero que indica el número de estratos en los que dividir los productos según su popularidad.\n",
    "  - num_samples: un entero que indica el número de muestras aleatorias que se tomarán de cada estrato para cada usuario.\n",
    "  - le_producto: un objeto de tipo LabelEncoder de scikit-learn que se usa para transformar los IDs de los productos.\n",
    "\n",
    "  Salida:\n",
    "  - Un objeto de tipo ndarray de numpy que representa la matriz de interacciones usuario-producto para aquellos productos que el usuario no ha interactuado previamente. Cada fila representa una interacción usuario-producto y los valores de cada columna representan el id del usuario y el id del producto, respectivamente.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # Definicion de mensaje inicial\n",
    "  mensaje_inicial=\"Iniciando muestreo estratificado basado en popularidad\"\n",
    "  print(mensaje_inicial)\n",
    "  print(\"-\"*len(mensaje_inicial), \"\\n\")\n",
    "\n",
    "  # Identificamos los usuarios presentes en test\n",
    "  usuarios_test = np.unique(test_x[:, 0])\n",
    "\n",
    "  # Identificamos el rango de productos disponibles\n",
    "  total_productos = range(dims_usuarios_productos[0]-1, dims_usuarios_productos[1])\n",
    "\n",
    "  # Calculamos la probabilidad como el numero de reviews por producto\n",
    "  product_popularity = df.groupby('asin').size().reset_index(name='popularity')\n",
    "  product_popularity[\"asin\"]=le_producto.transform(product_popularity[\"asin\"])\n",
    "  product_popularity.loc[:, col_names[\"col_id_product\"]] = product_popularity[col_names[\"col_id_product\"]].apply(lambda x: x + dims_usuarios_productos[0]).astype('int32')\n",
    "\n",
    "  # Definimos la funcion de densidad de donde vamos a samplear:\n",
    "  # Create the density plot\n",
    "  show_graph=True\n",
    "  if show_graph==True:\n",
    "    fig = ff.create_distplot([product_popularity['popularity']], ['Popularity'])\n",
    "    fig.show()\n",
    "  # Definimos el numero de estratos en los que dividir a os productos segun popularidad\n",
    "  #num_estratos = 5\n",
    "  product_popularity['stratum'] = pd.qcut(product_popularity['popularity'], num_estratos, labels=False)\n",
    "\n",
    "  # Ordenar los estratos por popularidad media\n",
    "  stratum_popularity = product_popularity.groupby('stratum')['popularity'].mean()\n",
    "  stratum_popularity = stratum_popularity.sort_values(ascending=False)\n",
    "\n",
    "  # Calcular las probabilidades de muestreo. Esta funcion puede ser definida de otra manera\n",
    "  stratum_probs = stratum_popularity / stratum_popularity.sum()\n",
    "  stratum_probs = stratum_probs.reset_index(drop=True)\n",
    "\n",
    "  print(\"Probabilidad de los estratos: \")\n",
    "  print( stratum_probs, \"\\n\")\n",
    "\n",
    "  # Definicion de sample por estrato\n",
    "  #num_samples = 199\n",
    "  stratum_test_sizes = (stratum_probs * num_samples)\n",
    "  stratum_test_sizes = np.round(stratum_test_sizes).astype(int)\n",
    "\n",
    "  # Si la suma de los tamaños de los estratos es mayor que num_samples, restamos uno al tamaño del estrato más grande\n",
    "  while stratum_test_sizes.sum() > num_samples:\n",
    "      stratum_test_sizes[stratum_test_sizes.argmin()] -= 1\n",
    "\n",
    "  # Si la suma de los tamaños de los estratos es menor que num_samples, sumamos uno al tamaño del estrato más grande\n",
    "  while stratum_test_sizes.sum() < num_samples:\n",
    "      stratum_test_sizes[stratum_test_sizes.argmax()] += 1\n",
    "\n",
    "  print(\"Tamaño de muestreo de los estratos de los estratos: \" )\n",
    "  print( stratum_test_sizes, \"\\n\")\n",
    "\n",
    "  # Definimos la matriz solucion y rellenamos\n",
    "  zero_positions = np.zeros((num_samples * len(usuarios_test), 2))\n",
    "  start_index = 0\n",
    "\n",
    "  # Iteramos sobre los usuarios de prueba\n",
    "  for usuario in tqdm(usuarios_test):\n",
    "    \n",
    "    # Iteramos sobre cada estrato y su tamaño correspondiente\n",
    "    for stratum, size in stratum_test_sizes.items():\n",
    "      \n",
    "      # Identificamos los productos en los que el usuario ha interactuado previamente en entrenamiento y el producto de gt\n",
    "      productos_train = np.unique(train_x[train_x[:, 0] == usuario][:, 1])\n",
    "      productos_test = np.unique(test_x[test_x[:, 0] == usuario][:, 1])\n",
    "      productos = np.concatenate((productos_train, productos_test))\n",
    "\n",
    "      # Identificamos los productos del estrato actual\n",
    "      products_in_stratum = product_popularity[product_popularity['stratum'] == stratum]['asin']\n",
    "\n",
    "      # Filtramos los productos del estrato actual que el usuario ha interactuado previamente\n",
    "      products_in_stratum_filtered = np.setdiff1d(products_in_stratum, productos)\n",
    "\n",
    "      # Tomamos una muestra aleatoria sin reemplazo de los productos filtrados del estrato actual\n",
    "      sampled_products = np.random.choice(products_in_stratum_filtered, size, replace=False)\n",
    "\n",
    "      # Asignamos los índices de inicio y fin para los ceros que representan la interacción usuario-producto\n",
    "      end_index = start_index + size\n",
    "      zero_positions[start_index:end_index, 0] = usuario\n",
    "      zero_positions[start_index:end_index, 1] = sampled_products\n",
    "      start_index = end_index\n",
    "\n",
    "  # Convertimos los valores de cero posiciones a entero\n",
    "  zero_positions = zero_positions.astype(int)\n",
    "\n",
    "  return(zero_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_positions_mode(mode, rating_mat, train_x, test_x, dims):\n",
    "   \n",
    "    if mode == 0:\n",
    "        save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data  \"+\"#\"*4)\n",
    "        return np.asarray(np.where(rating_mat.A==0)).T\n",
    "    elif mode == 1:\n",
    "        zero_true_matrix = np.where(rating_mat.A==0)\n",
    "        save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data separated by rows  \"+\"#\"*4)\n",
    "        return np.asarray([zero_true_matrix[0],zero_true_matrix[1]]).T\n",
    "    else:\n",
    "        save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: random sampling  \"+\"#\"*4)\n",
    "        zp = create_test_no_interactions(train_x, test_x, dims,  num_samples=199)\n",
    "        return zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14138/14138 [02:09<00:00, 109.03it/s]\n"
     ]
    }
   ],
   "source": [
    "zp = zero_positions_mode(2, rating_mat, train_x, test_x, dims)\n",
    "num_samples = 199\n",
    "num_estratos = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "le_product = LabelEncoder()\n",
    "le_product.fit(data[col_names[\"col_id_product\"]])\n",
    "zp = create_test_no_interactions_stratified_users(df, test_x, train_x, dims, num_estratos, num_samples, le_product, show_graph=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14138/14138 [01:50<00:00, 127.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2813462, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 16821],\n",
       "       [    0, 17652],\n",
       "       [    0, 16736],\n",
       "       ...,\n",
       "       [14137, 19744],\n",
       "       [14137, 16925],\n",
       "       [14137, 18058]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero_positions = np.asarray(np.where(rating_mat.A==0)).T\n",
    "zero_positions = zero_positions_mode(2, rating_mat, train_x, test_x, dims)\n",
    "print(save_data_configuration(str(zero_positions.shape)+\"\\n\"))\n",
    "zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14138/14138 [02:13<00:00, 105.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([16821, 17652, 16736, 17241, 18263, 20013, 17964, 18347, 15142,\n",
       "       16526, 18892, 15614, 18249, 16315, 19680, 16730, 17709, 15693,\n",
       "       20017, 17571, 15748, 20011, 18969, 19803, 15554, 16150, 15359,\n",
       "       19386, 17178, 17214, 19783, 14512, 17261, 14644, 18805, 15822,\n",
       "       17527, 15019, 18958, 15953, 19044, 15149, 19653, 14901, 15285,\n",
       "       16106, 18011, 19844, 14981, 18264, 14798, 16176, 15591, 15400,\n",
       "       19800, 18471, 16154, 18479, 14992, 17000, 14778, 15133, 18532,\n",
       "       19870, 16614, 17431, 15426, 15080, 18848, 18655, 14770, 19394,\n",
       "       19453, 15073, 17937, 20225, 16991, 17896, 15233, 19310, 17926,\n",
       "       14616, 16958, 19185, 15611, 19987, 19451, 20118, 18547, 16427,\n",
       "       16521, 17490, 18361, 14268, 17798, 19048, 19695, 14380, 18990,\n",
       "       15784, 18623, 16183, 19449, 17179, 16706, 14181, 19820, 15718,\n",
       "       14439, 17253, 16456, 16160, 16442, 14664, 18420, 19165, 16288,\n",
       "       15992, 15456, 16876, 20298, 17851, 15790, 18081, 14581, 15056,\n",
       "       19359, 19397, 17540, 18945, 18069, 19362, 18915, 18398, 19180,\n",
       "       17847, 15027, 14761, 15847, 18032, 14138, 15324, 16750, 18521,\n",
       "       16170, 14595, 16137, 15134, 15663, 15759, 14456, 18217, 16293,\n",
       "       17396, 19500, 17214, 19241, 14681, 19816, 18801, 15701, 15341,\n",
       "       16718, 20103, 14743, 17368, 17330, 17391, 19545, 14628, 14284,\n",
       "       17069, 15348, 17643, 15875, 19893, 19497, 19481, 19673, 19698,\n",
       "       19036, 15625, 15694, 16910, 19737, 17269, 17513, 19213, 16195,\n",
       "       16920, 17450, 16971, 14623, 18979, 18087, 15843, 17641, 16272,\n",
       "       17960])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items2compute = []\n",
    "for user in trange(dims[0]):\n",
    "    aux = zero_positions[zero_positions[:, 0] == user][:, 1]\n",
    "    items2compute.append(aux[aux >= dims[0]])\n",
    "items2compute[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING TEST SET...: 14138it [00:04, 3099.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249],\n",
       "       [    0, 16821],\n",
       "       [    0, 17652],\n",
       "       [    0, 16736],\n",
       "       [    0, 17241],\n",
       "       [    0, 18263],\n",
       "       [    0, 20013],\n",
       "       [    0, 17964],\n",
       "       [    0, 18347],\n",
       "       [    0, 15142],\n",
       "       [    0, 16526],\n",
       "       [    0, 18892],\n",
       "       [    0, 15614],\n",
       "       [    0, 18249],\n",
       "       [    0, 16315],\n",
       "       [    0, 19680],\n",
       "       [    0, 16730],\n",
       "       [    0, 17709],\n",
       "       [    0, 15693],\n",
       "       [    0, 20017],\n",
       "       [    0, 17571],\n",
       "       [    0, 15748],\n",
       "       [    0, 20011],\n",
       "       [    0, 18969],\n",
       "       [    0, 19803],\n",
       "       [    0, 15554],\n",
       "       [    0, 16150],\n",
       "       [    0, 15359],\n",
       "       [    0, 19386],\n",
       "       [    0, 17178],\n",
       "       [    0, 17214],\n",
       "       [    0, 19783],\n",
       "       [    0, 14512],\n",
       "       [    0, 17261],\n",
       "       [    0, 14644],\n",
       "       [    0, 18805],\n",
       "       [    0, 15822],\n",
       "       [    0, 17527],\n",
       "       [    0, 15019],\n",
       "       [    0, 18958],\n",
       "       [    0, 15953],\n",
       "       [    0, 19044],\n",
       "       [    0, 15149],\n",
       "       [    0, 19653],\n",
       "       [    0, 14901],\n",
       "       [    0, 15285],\n",
       "       [    0, 16106],\n",
       "       [    0, 18011],\n",
       "       [    0, 19844],\n",
       "       [    0, 14981],\n",
       "       [    0, 18264],\n",
       "       [    0, 14798],\n",
       "       [    0, 16176],\n",
       "       [    0, 15591],\n",
       "       [    0, 15400],\n",
       "       [    0, 19800],\n",
       "       [    0, 18471],\n",
       "       [    0, 16154],\n",
       "       [    0, 18479],\n",
       "       [    0, 14992],\n",
       "       [    0, 17000],\n",
       "       [    0, 14778],\n",
       "       [    0, 15133],\n",
       "       [    0, 18532],\n",
       "       [    0, 19870],\n",
       "       [    0, 16614],\n",
       "       [    0, 17431],\n",
       "       [    0, 15426],\n",
       "       [    0, 15080],\n",
       "       [    0, 18848],\n",
       "       [    0, 18655],\n",
       "       [    0, 14770],\n",
       "       [    0, 19394],\n",
       "       [    0, 19453],\n",
       "       [    0, 15073],\n",
       "       [    0, 17937],\n",
       "       [    0, 20225],\n",
       "       [    0, 16991],\n",
       "       [    0, 17896],\n",
       "       [    0, 15233],\n",
       "       [    0, 19310],\n",
       "       [    0, 17926],\n",
       "       [    0, 14616],\n",
       "       [    0, 16958],\n",
       "       [    0, 19185],\n",
       "       [    0, 15611],\n",
       "       [    0, 19987],\n",
       "       [    0, 19451],\n",
       "       [    0, 20118],\n",
       "       [    0, 18547],\n",
       "       [    0, 16427],\n",
       "       [    0, 16521],\n",
       "       [    0, 17490],\n",
       "       [    0, 18361],\n",
       "       [    0, 14268],\n",
       "       [    0, 17798],\n",
       "       [    0, 19048],\n",
       "       [    0, 19695],\n",
       "       [    0, 14380],\n",
       "       [    0, 18990],\n",
       "       [    0, 15784],\n",
       "       [    0, 18623],\n",
       "       [    0, 16183],\n",
       "       [    0, 19449],\n",
       "       [    0, 17179],\n",
       "       [    0, 16706],\n",
       "       [    0, 14181],\n",
       "       [    0, 19820],\n",
       "       [    0, 15718],\n",
       "       [    0, 14439],\n",
       "       [    0, 17253],\n",
       "       [    0, 16456],\n",
       "       [    0, 16160],\n",
       "       [    0, 16442],\n",
       "       [    0, 14664],\n",
       "       [    0, 18420],\n",
       "       [    0, 19165],\n",
       "       [    0, 16288],\n",
       "       [    0, 15992],\n",
       "       [    0, 15456],\n",
       "       [    0, 16876],\n",
       "       [    0, 20298],\n",
       "       [    0, 17851],\n",
       "       [    0, 15790],\n",
       "       [    0, 18081],\n",
       "       [    0, 14581],\n",
       "       [    0, 15056],\n",
       "       [    0, 19359],\n",
       "       [    0, 19397],\n",
       "       [    0, 17540],\n",
       "       [    0, 18945],\n",
       "       [    0, 18069],\n",
       "       [    0, 19362],\n",
       "       [    0, 18915],\n",
       "       [    0, 18398],\n",
       "       [    0, 19180],\n",
       "       [    0, 17847],\n",
       "       [    0, 15027],\n",
       "       [    0, 14761],\n",
       "       [    0, 15847],\n",
       "       [    0, 18032],\n",
       "       [    0, 14138],\n",
       "       [    0, 15324],\n",
       "       [    0, 16750],\n",
       "       [    0, 18521],\n",
       "       [    0, 16170],\n",
       "       [    0, 14595],\n",
       "       [    0, 16137],\n",
       "       [    0, 15134],\n",
       "       [    0, 15663],\n",
       "       [    0, 15759],\n",
       "       [    0, 14456],\n",
       "       [    0, 18217],\n",
       "       [    0, 16293],\n",
       "       [    0, 17396],\n",
       "       [    0, 19500],\n",
       "       [    0, 17214],\n",
       "       [    0, 19241],\n",
       "       [    0, 14681],\n",
       "       [    0, 19816],\n",
       "       [    0, 18801],\n",
       "       [    0, 15701],\n",
       "       [    0, 15341],\n",
       "       [    0, 16718],\n",
       "       [    0, 20103],\n",
       "       [    0, 14743],\n",
       "       [    0, 17368],\n",
       "       [    0, 17330],\n",
       "       [    0, 17391],\n",
       "       [    0, 19545],\n",
       "       [    0, 14628],\n",
       "       [    0, 14284],\n",
       "       [    0, 17069],\n",
       "       [    0, 15348],\n",
       "       [    0, 17643],\n",
       "       [    0, 15875],\n",
       "       [    0, 19893],\n",
       "       [    0, 19497],\n",
       "       [    0, 19481],\n",
       "       [    0, 19673],\n",
       "       [    0, 19698],\n",
       "       [    0, 19036],\n",
       "       [    0, 15625],\n",
       "       [    0, 15694],\n",
       "       [    0, 16910],\n",
       "       [    0, 19737],\n",
       "       [    0, 17269],\n",
       "       [    0, 17513],\n",
       "       [    0, 19213],\n",
       "       [    0, 16195],\n",
       "       [    0, 16920],\n",
       "       [    0, 17450],\n",
       "       [    0, 16971],\n",
       "       [    0, 14623],\n",
       "       [    0, 18979],\n",
       "       [    0, 18087],\n",
       "       [    0, 15843],\n",
       "       [    0, 17641],\n",
       "       [    0, 16272],\n",
       "       [    0, 17960]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_test_set(itemsnoninteracted:list, gt_test_interactions: np.ndarray) -> list:\n",
    "    #max_users, max_items = dims # number users (943), number items (2625)\n",
    "    test_set = []\n",
    "    for pair, negatives in tqdm(zip(gt_test_interactions, itemsnoninteracted), desc=\"BUILDING TEST SET...\"):\n",
    "        # APPEND TEST SETS FOR SINGLE USER\n",
    "        negatives = np.delete(negatives, np.where(negatives == pair[1]))\n",
    "        single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
    "        single_user_test_set[:, 1][1:] = negatives\n",
    "        test_set.append(single_user_test_set.copy())\n",
    "    return test_set\n",
    "\n",
    "test_x = build_test_set(items2compute, test_x)\n",
    "test_x[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Factorization Machines model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM_operation(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 reduce_sum: bool=True) -> None:\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        # square_of_sum = np.sum(x, dim=1) ** 2 # ...\n",
    "        # sum_of_square = np.sum(x ** 2, dim=1) # ...\n",
    "        \n",
    "        square_of_sum = torch.pow(torch.sum(x, dim=1),2)\n",
    "        sum_of_square = torch.sum(torch.pow(x,2), dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 field_dims: list,\n",
    "                 embed_dim: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(len(field_dims), 1)\n",
    "        self.embedding = torch.nn.Embedding(field_dims[-1], embed_dim)\n",
    "        self.fm = FM_operation(reduce_sum=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, interaction_pairs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        out = self.linear(interaction_pairs.float()) + self.fm(self.embedding(interaction_pairs))\n",
    "        return out.squeeze(1)\n",
    "        \n",
    "    def predict(self, \n",
    "                interactions: np.ndarray,\n",
    "                device: torch.device) -> torch.Tensor:\n",
    "        # return the score, inputs are numpy arrays, outputs are tensors\n",
    "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device) #, dtype=torch.long)\n",
    "        output_scores = self.forward(test_interactions)\n",
    "        return output_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    criterion: torch.nn.functional,\n",
    "                    device: torch.device) -> float:\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "\n",
    "    for i, (interactions, targets) in enumerate(data_loader):\n",
    "        interactions = interactions.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        predictions = model(interactions)\n",
    "    \n",
    "        loss = criterion(predictions, targets.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    return mean(total_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getHitRatio(recommend_list: list,\n",
    "                gt_item: int) -> bool:\n",
    "    if gt_item in recommend_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getNDCG(recommend_list: list,\n",
    "            gt_item: int) -> float:\n",
    "    idx = np.where(recommend_list == gt_item)[0]\n",
    "    if len(idx) > 0:\n",
    "        return math.log(2)/math.log(idx+2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module,\n",
    "         test_x: np.ndarray,\n",
    "         device: torch.device,\n",
    "         topk: int=10) -> Tuple[float, float]:\n",
    "    # Test the HR and NDCG for the model @topK\n",
    "    model.eval()\n",
    "\n",
    "    HR, NDCG = [], []\n",
    "    for user_test in test_x:\n",
    "        gt_item = user_test[0][1]\n",
    "        predictions = model.predict(user_test, device)\n",
    "        _, indices = torch.topk(predictions, topk)\n",
    "        recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
    "\n",
    "        HR.append(getHitRatio(recommend_list, gt_item))\n",
    "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
    "    return mean(HR), mean(NDCG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE\n",
    "## Defining the model, the loss and the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = train_dataset.dims\n",
    "model = FactorizationMachineModel(dims, hparams['hidden_size']).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=hparams['learning_rate'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: list) -> None:\n",
    "        super(RandomModel, self).__init__()\n",
    "        \"\"\"\n",
    "        Simple random based recommender system\n",
    "        \"\"\"\n",
    "        self.all_items = list(range(dims[0], dims[1]))\n",
    "\n",
    "    def forward(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def predict(self,\n",
    "                interactions: np.ndarray,\n",
    "                device=None) -> torch.Tensor:\n",
    "        return torch.FloatTensor(random.sample(self.all_items, len(interactions)))\n",
    "\n",
    "rnd_model = RandomModel(dims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      "training loss = 309.8556 | Eval: HR@10 = 0.0395, NDCG@10 = 0.0181 \n",
      "\n",
      "\n",
      "epoch 1:\n",
      "training loss = 0.5132 | Eval: HR@10 = 0.1502, NDCG@10 = 0.0949 \n",
      "\n",
      "\n",
      "epoch 2:\n",
      "training loss = 0.5054 | Eval: HR@10 = 0.1086, NDCG@10 = 0.0683 \n",
      "\n",
      "\n",
      "epoch 3:\n",
      "training loss = 0.4907 | Eval: HR@10 = 0.1318, NDCG@10 = 0.0809 \n",
      "\n",
      "\n",
      "epoch 4:\n",
      "training loss = 0.4683 | Eval: HR@10 = 0.1715, NDCG@10 = 0.1036 \n",
      "\n",
      "\n",
      "epoch 5:\n",
      "training loss = 0.4477 | Eval: HR@10 = 0.1937, NDCG@10 = 0.1162 \n",
      "\n",
      "\n",
      "epoch 6:\n",
      "training loss = 0.4291 | Eval: HR@10 = 0.2109, NDCG@10 = 0.1257 \n",
      "\n",
      "\n",
      "epoch 7:\n",
      "training loss = 0.4143 | Eval: HR@10 = 0.2266, NDCG@10 = 0.1335 \n",
      "\n",
      "\n",
      "epoch 8:\n",
      "training loss = 0.4038 | Eval: HR@10 = 0.2379, NDCG@10 = 0.1396 \n",
      "\n",
      "\n",
      "epoch 9:\n",
      "training loss = 0.3973 | Eval: HR@10 = 0.2465, NDCG@10 = 0.1442 \n",
      "\n",
      "\n",
      "epoch 10:\n",
      "training loss = 0.3923 | Eval: HR@10 = 0.2525, NDCG@10 = 0.1478 \n",
      "\n",
      "\n",
      "epoch 11:\n",
      "training loss = 0.3867 | Eval: HR@10 = 0.2494, NDCG@10 = 0.1461 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO EPOCHS NOW\n",
    "topk = 10\n",
    "for epoch_i in range(hparams['num_epochs']):\n",
    "    #data_loader.dataset.negative_sampling()\n",
    "    train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
    "    hr, ndcg = test(model, test_x, device, topk=topk)\n",
    "\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    print('\\n')\n",
    " \n",
    "    tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
    "    tb_fm.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_fm.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "    hr, ndcg = test(rnd_model, test_x, device, topk=topk)\n",
    "    tb_rnd.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_rnd.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"kill\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!kill 2820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs_random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977d1ca4f86cf6796d22eaa6050e2f72f1e03740aa8986f601710bf090b5ff9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
