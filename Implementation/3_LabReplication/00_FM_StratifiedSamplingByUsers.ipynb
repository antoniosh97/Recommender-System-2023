{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#====================== Import de librerias =====================#\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import wget\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from tqdm import tqdm, trange\n",
    "from IPython import embed\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys \n",
    "\n",
    "current_filename = os.listdir()[3].split(\".\")[-2][3:]\n",
    "logfile = \"project.log\"\n",
    "old_path = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "execution_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "logs_base_dir = \"runs_\"+current_filename\n",
    "os.environ[\"run_tensorboard\"] = logs_base_dir\n",
    "\n",
    "os.makedirs(f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}', exist_ok=True)\n",
    "tb_fm = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_FM/')\n",
    "tb_rnd = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_RANDOM/')\n",
    "\n",
    "def save_data_configuration(text):\n",
    "    save_data_dir = \"data_config_\" + current_filename +  \".txt\"\n",
    "    path = f'{execution_path}/{\"4_Modelling\"}/{save_data_dir}'\n",
    "    with open(path, \"a\") as data_file:\n",
    "        data_file.write(text+\"\\n\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some hyper-parameters\n",
    "hparams = {\n",
    "    'batch_size':64,\n",
    "    'num_epochs':12,\n",
    "    'hidden_size': 32,\n",
    "    'learning_rate':1e-4,\n",
    "}\n",
    "\n",
    "# we select to work on GPU if it is available in the machine, otherwise\n",
    "# will run on CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Definicion de valores de configuracion ============#\n",
    "\n",
    "min_reviews, min_usuarios = [6,6]\n",
    "col_names = {\"col_id_reviewer\": \"reviewerID\",\n",
    "             \"col_id_product\": \"asin\",\n",
    "             \"col_unix_time\": \"unixReviewTime\",\n",
    "             \"col_rating\": \"overall\",\n",
    "             \"col_timestamp\": \"timestamp\",\n",
    "             \"col_year\": \"year\"}\n",
    "\n",
    "csv_filename = execution_path/Path(\"3_DataPreparation/interactions_minR{}_minU{}.csv\".format(min_reviews,min_usuarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1477785600</td>\n",
       "      <td>2016-10-30 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10612</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1467244800</td>\n",
       "      <td>2016-06-30 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1454716800</td>\n",
       "      <td>2016-02-06 01:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1434844800</td>\n",
       "      <td>2015-06-21 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2523</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1420329600</td>\n",
       "      <td>2015-01-04 01:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asin  reviewerID  overall  unixReviewTime            timestamp  year\n",
       "0     0        9132      5.0      1477785600  2016-10-30 02:00:00  1970\n",
       "1     0       10612      5.0      1467244800  2016-06-30 02:00:00  1970\n",
       "2     0         257      1.0      1454716800  2016-02-06 01:00:00  1970\n",
       "3     0        4425      5.0      1434844800  2015-06-21 02:00:00  1970\n",
       "4     0        2523      4.0      1420329600  2015-01-04 01:00:00  1970"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin               6178\n",
       "reviewerID        14138\n",
       "overall               5\n",
       "unixReviewTime     3622\n",
       "timestamp          3622\n",
       "year                  1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_configuration(str(df.nunique()))\n",
    "df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset (TLOO strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data: np.ndarray,\n",
    "                     n_users: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Split and remove timestamp\n",
    "    train_x, test_x = [], []\n",
    "    for u in trange(n_users, desc='spliting train/test and removing timestamp...'):\n",
    "        user_data = data[data[:, 0] == u]\n",
    "        sorted_data = user_data[user_data[:, -1].argsort()]\n",
    "        if len(sorted_data) == 1:\n",
    "            train_x.append(sorted_data[0][:-1])\n",
    "        else:\n",
    "            train_x.append(sorted_data[:-1][:, :-1])\n",
    "            test_x.append(sorted_data[-1][:-1])\n",
    "    return np.vstack(train_x), np.stack(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      9132,          0, 1477785600],\n",
       "       [     10612,          0, 1467244800],\n",
       "       [       257,          0, 1454716800],\n",
       "       ...,\n",
       "       [      9051,       6177, 1530144000],\n",
       "       [      3412,       6177, 1527465600],\n",
       "       [      9805,       6177, 1527206400]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[*col_names.values()][:3]].astype('int32').to_numpy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim of users: 14138\n",
      "Dim of items: 20316\n",
      "Dims of unixtime: 1538006401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[      9132,      14138, 1477785600],\n",
       "       [     10612,      14138, 1467244800],\n",
       "       [       257,      14138, 1454716800],\n",
       "       ...,\n",
       "       [      9051,      20315, 1530144000],\n",
       "       [      3412,      20315, 1527465600],\n",
       "       [      9805,      20315, 1527206400]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_dims=0\n",
    "for i in range(data.shape[1] - 1):  # do not affect to timestamp\n",
    "    # MAKE IT START BY 0\n",
    "    data[:, i] -= np.min(data[:, i])\n",
    "    # RE-INDEX\n",
    "    data[:, i] += add_dims\n",
    "    add_dims = np.max(data[:, i]) + 1\n",
    "dims = np.max(data, axis=0) + 1\n",
    "print(\"Dim of users: {}\\nDim of items: {}\\nDims of unixtime: {}\".format(dims[0], dims[1], dims[2]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting train/test and removing timestamp...: 100%|██████████| 14138/14138 [00:04<00:00, 2917.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248],\n",
       "       [    0, 19249],\n",
       "       [    0, 14823],\n",
       "       ...,\n",
       "       [14137, 14159],\n",
       "       [14137, 18245],\n",
       "       [14137, 18904]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x = split_train_test(data, dims[0])\n",
    "train_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dims: [14138 20316]\n",
      "New train_x:\n",
      " [[    0 19248]\n",
      " [    0 19249]\n",
      " [    0 14823]\n",
      " ...\n",
      " [14137 14159]\n",
      " [14137 18245]\n",
      " [14137 18904]]\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x[:, :2]\n",
    "dims = dims[:2]\n",
    "print(\"New dims:\",dims)\n",
    "print(\"New train_x:\\n\",train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adj_mx(n_feat:int, data:np.ndarray) -> sp.dok_matrix :\n",
    "    train_mat = sp.dok_matrix((n_feat, n_feat), dtype=np.float32)\n",
    "    for x in tqdm(data, desc=f\"BUILDING ADJACENCY MATRIX...\"):\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "        train_mat[x[1], x[0]] = 1.0\n",
    "        # IDEA: We treat features that are not user or item differently because we do not consider\n",
    "        #  interactions between contexts\n",
    "        if data.shape[1] > 2:\n",
    "            for idx in range(len(x[2:])):\n",
    "                train_mat[x[0], x[2 + idx]] = 1.0\n",
    "                train_mat[x[1], x[2 + idx]] = 1.0\n",
    "                train_mat[x[2 + idx], x[0]] = 1.0\n",
    "                train_mat[x[2 + idx], x[1]] = 1.0\n",
    "    return train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ng_sample(data: np.ndarray, dims: list, num_ng:int=4) -> Tuple[np.ndarray, sp.dok_matrix]:\n",
    "    rating_mat = build_adj_mx(dims[-1], data)\n",
    "    interactions = []\n",
    "    min_item, max_item = dims[0], dims[1]\n",
    "    for num, x in tqdm(enumerate(data), desc='perform negative sampling...'):\n",
    "        interactions.append(np.append(x, 1))\n",
    "        for t in range(num_ng):\n",
    "            j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
    "            while (x[0], j) in rating_mat or j == int(x[1]):\n",
    "                j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            interactions.append(np.concatenate([[x[0], j], x[2:], [0]]))\n",
    "    return np.vstack(interactions), rating_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING ADJACENCY MATRIX...: 100%|██████████| 123226/123226 [00:03<00:00, 40789.19it/s]\n",
      "perform negative sampling...: 123226it [00:05, 22110.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions matrix:\n",
      " [14138 20316]\n",
      "\n",
      "Rating matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<20316x20316 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 246452 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, rating_mat = ng_sample(train_x, dims)\n",
    "print(\"Dimensions matrix:\\n\",dims)\n",
    "print(\"\\nRating matrix:\")\n",
    "rating_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6178"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims[-1]-dims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000597112191656141\n",
      "0.9994028878083439\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Evaristo\n",
    "#### number of ones\n",
    "print(np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "### number of zeros\n",
    "print(1 - np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "\n",
    "# ## Brenda\n",
    "# #### Who sparse is the matrix??\n",
    "# print(1 - rating_mat.shape[0] / rating_mat.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248,     1],\n",
       "       [    0, 16528,     0],\n",
       "       [    0, 17066,     0],\n",
       "       [    0, 17342,     0],\n",
       "       [    0, 14381,     0],\n",
       "       [    0, 19249,     1],\n",
       "       [    0, 18424,     0],\n",
       "       [    0, 19834,     0],\n",
       "       [    0, 17237,     0],\n",
       "       [    0, 19656,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointData(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 dims: list) -> None:\n",
    "        \"\"\"\n",
    "        Dataset formatter adapted point-wise algorithms\n",
    "        Parameters\n",
    "        \"\"\"\n",
    "        super(PointData, self).__init__()\n",
    "        self.interactions = data\n",
    "        self.dims = dims\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.interactions)\n",
    "        \n",
    "    def __getitem__(self, \n",
    "                    index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return the pairs user-item and the target.\n",
    "        \"\"\"\n",
    "        return self.interactions[index][:-1], self.interactions[index][-1]\n",
    "\n",
    "train_dataset = PointData(train_x, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0, 19248]), 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the test set for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249],\n",
       "       [    1, 18015],\n",
       "       [    2, 14196],\n",
       "       ...,\n",
       "       [14135, 19938],\n",
       "       [14136, 20214],\n",
       "       [14137, 15542]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20316, 20316)\n",
      "rating_mat contains log2(rating_mat.shape[0]) = 15 bits\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(rating_mat.shape)\n",
    "bits = math.ceil(math.log(rating_mat.shape[0],2))\n",
    "print(\"rating_mat contains log2(rating_mat.shape[0]) = {} bits\".format(bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_no_interactions_stratified_users(train_x: np.ndarray, test_x: np.ndarray, dims_usuarios_productos: Tuple[int, int],  num_samples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Esta funcion se encarga de crear de manera eficiente un dataset que contenga las interacciones usuario-producto en test que no se hayan producido.\n",
    "    \n",
    "    Argumentos:\n",
    "        train_x (np.ndarray): matriz de entrenamiento con las interacciones usuario-producto previas\n",
    "        test_x (np.ndarray): matriz de prueba con las interacciones usuario-producto previas\n",
    "        dims_usuarios_productos (Tuple[int, int]): rango de productos y usuarios disponibles\n",
    "    \n",
    "    Retorno:\n",
    "        np.ndarray: una matriz con todas las interacciones usuario-producto en test que no se hayan producido\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import random\n",
    "    \n",
    "    seed = 2   # get same results\n",
    "    random.seed(a = seed)\n",
    "    # Identificamos los usuarios presentes en la prueba\n",
    "    usuarios_test = np.unique(test_x[:, 0])\n",
    "    # Identificamos el rango de productos disponibles\n",
    "    total_productos = range(dims_usuarios_productos[0]-1, dims_usuarios_productos[1])\n",
    "    \n",
    "    # Recorremos cada usuario presente en la prueba\n",
    "    for usuario in tqdm(usuarios_test):\n",
    "        # Identificamos los productos en los que el usuario ha interactuado previamente en entrenamiento\n",
    "        productos_train = np.unique(train_x[train_x[:, 0] == usuario][:, 1])\n",
    "        # Seleccionamos al azar 199 productos con los que el usuario no ha interactuado previamente\n",
    "        productos_a_machear = random.choices(list(set(total_productos) - set(productos_train)), k=num_samples)\n",
    "        # Creamos una lista de interacciones usuario-producto para este usuario\n",
    "        # lista_por_usuario = [[usuario, x] for x in productos_a_machear]\n",
    "        # We must avoid using for loops!\n",
    "        lista_por_usuario = np.vstack([(np.ones(len(productos_a_machear))*usuario).astype(int), productos_a_machear]).T\n",
    "        \n",
    "        # Si es el primer usuario, inicializamos una matriz con sus interacciones\n",
    "        if usuario == 0:\n",
    "            zero_positions = lista_por_usuario\n",
    "        # Si no es el primer usuario, concatenamos sus interacciones a la matriz existente\n",
    "        else:\n",
    "            zero_positions = np.concatenate((zero_positions, lista_por_usuario), axis=0)\n",
    "            \n",
    "    return zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zero_positions_mode(mode, rating_mat, train_x, test_x, dims):\n",
    "   \n",
    "#     if mode == 0:\n",
    "#         save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data  \"+\"#\"*4)\n",
    "#         return np.asarray(np.where(rating_mat.A==0)).T\n",
    "#     elif mode == 1:\n",
    "#         zero_true_matrix = np.where(rating_mat.A==0)\n",
    "#         save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data separated by rows  \"+\"#\"*4)\n",
    "#         return np.asarray([zero_true_matrix[0],zero_true_matrix[1]]).T\n",
    "#     else:\n",
    "#         save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: random sampling  \"+\"#\"*4)\n",
    "#         zp = create_test_no_interactions(train_x, test_x, dims,  num_samples=199)\n",
    "#         return zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zp = zero_positions_mode(2, rating_mat, train_x, test_x, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14138/14138 [01:44<00:00, 134.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2813462, 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 14448],\n",
       "       [    0, 20227],\n",
       "       [    0, 18051],\n",
       "       ...,\n",
       "       [14137, 15688],\n",
       "       [14137, 19466],\n",
       "       [14137, 18571]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero_positions = np.asarray(np.where(rating_mat.A==0)).T\n",
    "save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: random sampling  \"+\"#\"*4)\n",
    "zero_positions = create_test_no_interactions_stratified_users(train_x, test_x, dims,  num_samples=199)\n",
    "print(save_data_configuration(str(zero_positions.shape)+\"\\n\"))\n",
    "zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14138/14138 [02:26<00:00, 96.60it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14448, 20227, 18051, 17816, 15307, 15422, 16102, 19843, 18101,\n",
       "       15226, 20092, 19105, 18459, 14282, 17910, 19374, 18485, 17528,\n",
       "       17900, 19059, 14328, 18597, 16772, 15672, 18652, 14259, 15393,\n",
       "       15491, 17337, 16549, 16356, 14797, 20164, 14885, 18759, 17676,\n",
       "       16784, 14947, 15412, 16697, 16607, 16740, 16146, 14222, 19641,\n",
       "       20110, 16271, 17935, 15595, 15488, 18508, 17240, 18610, 15204,\n",
       "       15575, 14422, 17375, 14937, 17162, 16581, 16410, 16964, 19394,\n",
       "       16873, 17947, 19523, 14549, 14392, 17538, 17623, 17843, 16529,\n",
       "       14819, 14413, 16138, 15857, 16418, 16590, 20052, 18460, 18973,\n",
       "       15309, 16182, 16071, 15238, 14955, 16848, 18547, 17292, 16517,\n",
       "       17902, 20063, 18220, 19913, 14954, 19590, 14332, 15996, 20197,\n",
       "       15787, 18294, 14988, 16051, 16552, 20001, 17209, 15479, 18401,\n",
       "       15681, 17388, 19760, 20221, 14575, 19350, 17819, 15336, 15850,\n",
       "       17911, 14491, 19673, 18262, 19385, 17472, 18891, 18016, 18251,\n",
       "       16609, 15891, 19504, 19182, 19330, 17753, 14291, 15461, 16705,\n",
       "       18456, 18520, 17476, 15773, 18897, 18728, 14961, 14589, 14941,\n",
       "       18591, 16365, 15764, 18800, 19681, 15250, 20019, 16082, 18841,\n",
       "       16509, 18251, 18428, 15949, 19275, 17725, 20199, 20290, 19497,\n",
       "       20213, 16282, 19708, 15844, 16841, 18919, 14444, 14187, 15333,\n",
       "       16757, 20154, 20081, 17491, 16287, 19715, 19484, 19127, 18986,\n",
       "       19499, 14687, 18672, 17160, 17742, 14697, 16144, 16943, 16872,\n",
       "       17796, 14762, 16118, 17807, 14954, 18233, 14293, 18573, 16794,\n",
       "       15790])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta parte del código se usaba porque el zero positions venia de la rating matrix la cual tenia informacion no útil como (user,user) o (item,item), por eso nos quedábamos con items mayores a dims[0]\n",
    "Ya que si recordamos, la matrix tenia size: (users + items)\n",
    "rango items iniciales = 0 ... 6177\n",
    "rango items actuales  = 14138 ... 20135\n",
    "\"\"\"\n",
    "\n",
    "# items2compute = []\n",
    "# for user in trange(dims[0]):\n",
    "#     aux = zero_positions[zero_positions[:, 0] == user][:, 1]\n",
    "#     items2compute.append(aux[aux >= dims[0]])\n",
    "# items2compute[0]\n",
    "\n",
    "# MUY RÁPIDO !!!!!\n",
    "items2compute = []\n",
    "start = 0\n",
    "num_samples = 199\n",
    "for user in trange(dims[0]):\n",
    "    aux = zero_positions[start:start+num_samples,1]\n",
    "    start += num_samples \n",
    "    items2compute.append(aux)\n",
    "items2compute[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING TEST SET...: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [    0 17249]\n",
      "5 [    0 17249]\n",
      "6 [[    0 17249]\n",
      " [    0 14448]\n",
      " [    0 20227]\n",
      " [    0 18051]\n",
      " [    0 17816]\n",
      " [    0 15307]\n",
      " [    0 15422]\n",
      " [    0 16102]\n",
      " [    0 19843]\n",
      " [    0 18101]\n",
      " [    0 15226]\n",
      " [    0 20092]\n",
      " [    0 19105]\n",
      " [    0 18459]\n",
      " [    0 14282]\n",
      " [    0 17910]\n",
      " [    0 19374]\n",
      " [    0 18485]\n",
      " [    0 17528]\n",
      " [    0 17900]\n",
      " [    0 19059]\n",
      " [    0 14328]\n",
      " [    0 18597]\n",
      " [    0 16772]\n",
      " [    0 15672]\n",
      " [    0 18652]\n",
      " [    0 14259]\n",
      " [    0 15393]\n",
      " [    0 15491]\n",
      " [    0 17337]\n",
      " [    0 16549]\n",
      " [    0 16356]\n",
      " [    0 14797]\n",
      " [    0 20164]\n",
      " [    0 14885]\n",
      " [    0 18759]\n",
      " [    0 17676]\n",
      " [    0 16784]\n",
      " [    0 14947]\n",
      " [    0 15412]\n",
      " [    0 16697]\n",
      " [    0 16607]\n",
      " [    0 16740]\n",
      " [    0 16146]\n",
      " [    0 14222]\n",
      " [    0 19641]\n",
      " [    0 20110]\n",
      " [    0 16271]\n",
      " [    0 17935]\n",
      " [    0 15595]\n",
      " [    0 15488]\n",
      " [    0 18508]\n",
      " [    0 17240]\n",
      " [    0 18610]\n",
      " [    0 15204]\n",
      " [    0 15575]\n",
      " [    0 14422]\n",
      " [    0 17375]\n",
      " [    0 14937]\n",
      " [    0 17162]\n",
      " [    0 16581]\n",
      " [    0 16410]\n",
      " [    0 16964]\n",
      " [    0 19394]\n",
      " [    0 16873]\n",
      " [    0 17947]\n",
      " [    0 19523]\n",
      " [    0 14549]\n",
      " [    0 14392]\n",
      " [    0 17538]\n",
      " [    0 17623]\n",
      " [    0 17843]\n",
      " [    0 16529]\n",
      " [    0 14819]\n",
      " [    0 14413]\n",
      " [    0 16138]\n",
      " [    0 15857]\n",
      " [    0 16418]\n",
      " [    0 16590]\n",
      " [    0 20052]\n",
      " [    0 18460]\n",
      " [    0 18973]\n",
      " [    0 15309]\n",
      " [    0 16182]\n",
      " [    0 16071]\n",
      " [    0 15238]\n",
      " [    0 14955]\n",
      " [    0 16848]\n",
      " [    0 18547]\n",
      " [    0 17292]\n",
      " [    0 16517]\n",
      " [    0 17902]\n",
      " [    0 20063]\n",
      " [    0 18220]\n",
      " [    0 19913]\n",
      " [    0 14954]\n",
      " [    0 19590]\n",
      " [    0 14332]\n",
      " [    0 15996]\n",
      " [    0 20197]\n",
      " [    0 15787]\n",
      " [    0 18294]\n",
      " [    0 14988]\n",
      " [    0 16051]\n",
      " [    0 16552]\n",
      " [    0 20001]\n",
      " [    0 17209]\n",
      " [    0 15479]\n",
      " [    0 18401]\n",
      " [    0 15681]\n",
      " [    0 17388]\n",
      " [    0 19760]\n",
      " [    0 20221]\n",
      " [    0 14575]\n",
      " [    0 19350]\n",
      " [    0 17819]\n",
      " [    0 15336]\n",
      " [    0 15850]\n",
      " [    0 17911]\n",
      " [    0 14491]\n",
      " [    0 19673]\n",
      " [    0 18262]\n",
      " [    0 19385]\n",
      " [    0 17472]\n",
      " [    0 18891]\n",
      " [    0 18016]\n",
      " [    0 18251]\n",
      " [    0 16609]\n",
      " [    0 15891]\n",
      " [    0 19504]\n",
      " [    0 19182]\n",
      " [    0 19330]\n",
      " [    0 17753]\n",
      " [    0 14291]\n",
      " [    0 15461]\n",
      " [    0 16705]\n",
      " [    0 18456]\n",
      " [    0 18520]\n",
      " [    0 17476]\n",
      " [    0 15773]\n",
      " [    0 18897]\n",
      " [    0 18728]\n",
      " [    0 14961]\n",
      " [    0 14589]\n",
      " [    0 14941]\n",
      " [    0 18591]\n",
      " [    0 16365]\n",
      " [    0 15764]\n",
      " [    0 18800]\n",
      " [    0 19681]\n",
      " [    0 15250]\n",
      " [    0 20019]\n",
      " [    0 16082]\n",
      " [    0 18841]\n",
      " [    0 16509]\n",
      " [    0 18251]\n",
      " [    0 18428]\n",
      " [    0 15949]\n",
      " [    0 19275]\n",
      " [    0 17725]\n",
      " [    0 20199]\n",
      " [    0 20290]\n",
      " [    0 19497]\n",
      " [    0 20213]\n",
      " [    0 16282]\n",
      " [    0 19708]\n",
      " [    0 15844]\n",
      " [    0 16841]\n",
      " [    0 18919]\n",
      " [    0 14444]\n",
      " [    0 14187]\n",
      " [    0 15333]\n",
      " [    0 16757]\n",
      " [    0 20154]\n",
      " [    0 20081]\n",
      " [    0 17491]\n",
      " [    0 16287]\n",
      " [    0 19715]\n",
      " [    0 19484]\n",
      " [    0 19127]\n",
      " [    0 18986]\n",
      " [    0 19499]\n",
      " [    0 14687]\n",
      " [    0 18672]\n",
      " [    0 17160]\n",
      " [    0 17742]\n",
      " [    0 14697]\n",
      " [    0 16144]\n",
      " [    0 16943]\n",
      " [    0 16872]\n",
      " [    0 17796]\n",
      " [    0 14762]\n",
      " [    0 16118]\n",
      " [    0 17807]\n",
      " [    0 14954]\n",
      " [    0 18233]\n",
      " [    0 14293]\n",
      " [    0 18573]\n",
      " [    0 16794]\n",
      " [    0 15790]]\n",
      "7 [array([[    0, 17249],\n",
      "       [    0, 14448],\n",
      "       [    0, 20227],\n",
      "       [    0, 18051],\n",
      "       [    0, 17816],\n",
      "       [    0, 15307],\n",
      "       [    0, 15422],\n",
      "       [    0, 16102],\n",
      "       [    0, 19843],\n",
      "       [    0, 18101],\n",
      "       [    0, 15226],\n",
      "       [    0, 20092],\n",
      "       [    0, 19105],\n",
      "       [    0, 18459],\n",
      "       [    0, 14282],\n",
      "       [    0, 17910],\n",
      "       [    0, 19374],\n",
      "       [    0, 18485],\n",
      "       [    0, 17528],\n",
      "       [    0, 17900],\n",
      "       [    0, 19059],\n",
      "       [    0, 14328],\n",
      "       [    0, 18597],\n",
      "       [    0, 16772],\n",
      "       [    0, 15672],\n",
      "       [    0, 18652],\n",
      "       [    0, 14259],\n",
      "       [    0, 15393],\n",
      "       [    0, 15491],\n",
      "       [    0, 17337],\n",
      "       [    0, 16549],\n",
      "       [    0, 16356],\n",
      "       [    0, 14797],\n",
      "       [    0, 20164],\n",
      "       [    0, 14885],\n",
      "       [    0, 18759],\n",
      "       [    0, 17676],\n",
      "       [    0, 16784],\n",
      "       [    0, 14947],\n",
      "       [    0, 15412],\n",
      "       [    0, 16697],\n",
      "       [    0, 16607],\n",
      "       [    0, 16740],\n",
      "       [    0, 16146],\n",
      "       [    0, 14222],\n",
      "       [    0, 19641],\n",
      "       [    0, 20110],\n",
      "       [    0, 16271],\n",
      "       [    0, 17935],\n",
      "       [    0, 15595],\n",
      "       [    0, 15488],\n",
      "       [    0, 18508],\n",
      "       [    0, 17240],\n",
      "       [    0, 18610],\n",
      "       [    0, 15204],\n",
      "       [    0, 15575],\n",
      "       [    0, 14422],\n",
      "       [    0, 17375],\n",
      "       [    0, 14937],\n",
      "       [    0, 17162],\n",
      "       [    0, 16581],\n",
      "       [    0, 16410],\n",
      "       [    0, 16964],\n",
      "       [    0, 19394],\n",
      "       [    0, 16873],\n",
      "       [    0, 17947],\n",
      "       [    0, 19523],\n",
      "       [    0, 14549],\n",
      "       [    0, 14392],\n",
      "       [    0, 17538],\n",
      "       [    0, 17623],\n",
      "       [    0, 17843],\n",
      "       [    0, 16529],\n",
      "       [    0, 14819],\n",
      "       [    0, 14413],\n",
      "       [    0, 16138],\n",
      "       [    0, 15857],\n",
      "       [    0, 16418],\n",
      "       [    0, 16590],\n",
      "       [    0, 20052],\n",
      "       [    0, 18460],\n",
      "       [    0, 18973],\n",
      "       [    0, 15309],\n",
      "       [    0, 16182],\n",
      "       [    0, 16071],\n",
      "       [    0, 15238],\n",
      "       [    0, 14955],\n",
      "       [    0, 16848],\n",
      "       [    0, 18547],\n",
      "       [    0, 17292],\n",
      "       [    0, 16517],\n",
      "       [    0, 17902],\n",
      "       [    0, 20063],\n",
      "       [    0, 18220],\n",
      "       [    0, 19913],\n",
      "       [    0, 14954],\n",
      "       [    0, 19590],\n",
      "       [    0, 14332],\n",
      "       [    0, 15996],\n",
      "       [    0, 20197],\n",
      "       [    0, 15787],\n",
      "       [    0, 18294],\n",
      "       [    0, 14988],\n",
      "       [    0, 16051],\n",
      "       [    0, 16552],\n",
      "       [    0, 20001],\n",
      "       [    0, 17209],\n",
      "       [    0, 15479],\n",
      "       [    0, 18401],\n",
      "       [    0, 15681],\n",
      "       [    0, 17388],\n",
      "       [    0, 19760],\n",
      "       [    0, 20221],\n",
      "       [    0, 14575],\n",
      "       [    0, 19350],\n",
      "       [    0, 17819],\n",
      "       [    0, 15336],\n",
      "       [    0, 15850],\n",
      "       [    0, 17911],\n",
      "       [    0, 14491],\n",
      "       [    0, 19673],\n",
      "       [    0, 18262],\n",
      "       [    0, 19385],\n",
      "       [    0, 17472],\n",
      "       [    0, 18891],\n",
      "       [    0, 18016],\n",
      "       [    0, 18251],\n",
      "       [    0, 16609],\n",
      "       [    0, 15891],\n",
      "       [    0, 19504],\n",
      "       [    0, 19182],\n",
      "       [    0, 19330],\n",
      "       [    0, 17753],\n",
      "       [    0, 14291],\n",
      "       [    0, 15461],\n",
      "       [    0, 16705],\n",
      "       [    0, 18456],\n",
      "       [    0, 18520],\n",
      "       [    0, 17476],\n",
      "       [    0, 15773],\n",
      "       [    0, 18897],\n",
      "       [    0, 18728],\n",
      "       [    0, 14961],\n",
      "       [    0, 14589],\n",
      "       [    0, 14941],\n",
      "       [    0, 18591],\n",
      "       [    0, 16365],\n",
      "       [    0, 15764],\n",
      "       [    0, 18800],\n",
      "       [    0, 19681],\n",
      "       [    0, 15250],\n",
      "       [    0, 20019],\n",
      "       [    0, 16082],\n",
      "       [    0, 18841],\n",
      "       [    0, 16509],\n",
      "       [    0, 18251],\n",
      "       [    0, 18428],\n",
      "       [    0, 15949],\n",
      "       [    0, 19275],\n",
      "       [    0, 17725],\n",
      "       [    0, 20199],\n",
      "       [    0, 20290],\n",
      "       [    0, 19497],\n",
      "       [    0, 20213],\n",
      "       [    0, 16282],\n",
      "       [    0, 19708],\n",
      "       [    0, 15844],\n",
      "       [    0, 16841],\n",
      "       [    0, 18919],\n",
      "       [    0, 14444],\n",
      "       [    0, 14187],\n",
      "       [    0, 15333],\n",
      "       [    0, 16757],\n",
      "       [    0, 20154],\n",
      "       [    0, 20081],\n",
      "       [    0, 17491],\n",
      "       [    0, 16287],\n",
      "       [    0, 19715],\n",
      "       [    0, 19484],\n",
      "       [    0, 19127],\n",
      "       [    0, 18986],\n",
      "       [    0, 19499],\n",
      "       [    0, 14687],\n",
      "       [    0, 18672],\n",
      "       [    0, 17160],\n",
      "       [    0, 17742],\n",
      "       [    0, 14697],\n",
      "       [    0, 16144],\n",
      "       [    0, 16943],\n",
      "       [    0, 16872],\n",
      "       [    0, 17796],\n",
      "       [    0, 14762],\n",
      "       [    0, 16118],\n",
      "       [    0, 17807],\n",
      "       [    0, 14954],\n",
      "       [    0, 18233],\n",
      "       [    0, 14293],\n",
      "       [    0, 18573],\n",
      "       [    0, 16794],\n",
      "       [    0, 15790]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[    0, 17249],\n",
       "        [    0, 14448],\n",
       "        [    0, 20227],\n",
       "        [    0, 18051],\n",
       "        [    0, 17816],\n",
       "        [    0, 15307],\n",
       "        [    0, 15422],\n",
       "        [    0, 16102],\n",
       "        [    0, 19843],\n",
       "        [    0, 18101],\n",
       "        [    0, 15226],\n",
       "        [    0, 20092],\n",
       "        [    0, 19105],\n",
       "        [    0, 18459],\n",
       "        [    0, 14282],\n",
       "        [    0, 17910],\n",
       "        [    0, 19374],\n",
       "        [    0, 18485],\n",
       "        [    0, 17528],\n",
       "        [    0, 17900],\n",
       "        [    0, 19059],\n",
       "        [    0, 14328],\n",
       "        [    0, 18597],\n",
       "        [    0, 16772],\n",
       "        [    0, 15672],\n",
       "        [    0, 18652],\n",
       "        [    0, 14259],\n",
       "        [    0, 15393],\n",
       "        [    0, 15491],\n",
       "        [    0, 17337],\n",
       "        [    0, 16549],\n",
       "        [    0, 16356],\n",
       "        [    0, 14797],\n",
       "        [    0, 20164],\n",
       "        [    0, 14885],\n",
       "        [    0, 18759],\n",
       "        [    0, 17676],\n",
       "        [    0, 16784],\n",
       "        [    0, 14947],\n",
       "        [    0, 15412],\n",
       "        [    0, 16697],\n",
       "        [    0, 16607],\n",
       "        [    0, 16740],\n",
       "        [    0, 16146],\n",
       "        [    0, 14222],\n",
       "        [    0, 19641],\n",
       "        [    0, 20110],\n",
       "        [    0, 16271],\n",
       "        [    0, 17935],\n",
       "        [    0, 15595],\n",
       "        [    0, 15488],\n",
       "        [    0, 18508],\n",
       "        [    0, 17240],\n",
       "        [    0, 18610],\n",
       "        [    0, 15204],\n",
       "        [    0, 15575],\n",
       "        [    0, 14422],\n",
       "        [    0, 17375],\n",
       "        [    0, 14937],\n",
       "        [    0, 17162],\n",
       "        [    0, 16581],\n",
       "        [    0, 16410],\n",
       "        [    0, 16964],\n",
       "        [    0, 19394],\n",
       "        [    0, 16873],\n",
       "        [    0, 17947],\n",
       "        [    0, 19523],\n",
       "        [    0, 14549],\n",
       "        [    0, 14392],\n",
       "        [    0, 17538],\n",
       "        [    0, 17623],\n",
       "        [    0, 17843],\n",
       "        [    0, 16529],\n",
       "        [    0, 14819],\n",
       "        [    0, 14413],\n",
       "        [    0, 16138],\n",
       "        [    0, 15857],\n",
       "        [    0, 16418],\n",
       "        [    0, 16590],\n",
       "        [    0, 20052],\n",
       "        [    0, 18460],\n",
       "        [    0, 18973],\n",
       "        [    0, 15309],\n",
       "        [    0, 16182],\n",
       "        [    0, 16071],\n",
       "        [    0, 15238],\n",
       "        [    0, 14955],\n",
       "        [    0, 16848],\n",
       "        [    0, 18547],\n",
       "        [    0, 17292],\n",
       "        [    0, 16517],\n",
       "        [    0, 17902],\n",
       "        [    0, 20063],\n",
       "        [    0, 18220],\n",
       "        [    0, 19913],\n",
       "        [    0, 14954],\n",
       "        [    0, 19590],\n",
       "        [    0, 14332],\n",
       "        [    0, 15996],\n",
       "        [    0, 20197],\n",
       "        [    0, 15787],\n",
       "        [    0, 18294],\n",
       "        [    0, 14988],\n",
       "        [    0, 16051],\n",
       "        [    0, 16552],\n",
       "        [    0, 20001],\n",
       "        [    0, 17209],\n",
       "        [    0, 15479],\n",
       "        [    0, 18401],\n",
       "        [    0, 15681],\n",
       "        [    0, 17388],\n",
       "        [    0, 19760],\n",
       "        [    0, 20221],\n",
       "        [    0, 14575],\n",
       "        [    0, 19350],\n",
       "        [    0, 17819],\n",
       "        [    0, 15336],\n",
       "        [    0, 15850],\n",
       "        [    0, 17911],\n",
       "        [    0, 14491],\n",
       "        [    0, 19673],\n",
       "        [    0, 18262],\n",
       "        [    0, 19385],\n",
       "        [    0, 17472],\n",
       "        [    0, 18891],\n",
       "        [    0, 18016],\n",
       "        [    0, 18251],\n",
       "        [    0, 16609],\n",
       "        [    0, 15891],\n",
       "        [    0, 19504],\n",
       "        [    0, 19182],\n",
       "        [    0, 19330],\n",
       "        [    0, 17753],\n",
       "        [    0, 14291],\n",
       "        [    0, 15461],\n",
       "        [    0, 16705],\n",
       "        [    0, 18456],\n",
       "        [    0, 18520],\n",
       "        [    0, 17476],\n",
       "        [    0, 15773],\n",
       "        [    0, 18897],\n",
       "        [    0, 18728],\n",
       "        [    0, 14961],\n",
       "        [    0, 14589],\n",
       "        [    0, 14941],\n",
       "        [    0, 18591],\n",
       "        [    0, 16365],\n",
       "        [    0, 15764],\n",
       "        [    0, 18800],\n",
       "        [    0, 19681],\n",
       "        [    0, 15250],\n",
       "        [    0, 20019],\n",
       "        [    0, 16082],\n",
       "        [    0, 18841],\n",
       "        [    0, 16509],\n",
       "        [    0, 18251],\n",
       "        [    0, 18428],\n",
       "        [    0, 15949],\n",
       "        [    0, 19275],\n",
       "        [    0, 17725],\n",
       "        [    0, 20199],\n",
       "        [    0, 20290],\n",
       "        [    0, 19497],\n",
       "        [    0, 20213],\n",
       "        [    0, 16282],\n",
       "        [    0, 19708],\n",
       "        [    0, 15844],\n",
       "        [    0, 16841],\n",
       "        [    0, 18919],\n",
       "        [    0, 14444],\n",
       "        [    0, 14187],\n",
       "        [    0, 15333],\n",
       "        [    0, 16757],\n",
       "        [    0, 20154],\n",
       "        [    0, 20081],\n",
       "        [    0, 17491],\n",
       "        [    0, 16287],\n",
       "        [    0, 19715],\n",
       "        [    0, 19484],\n",
       "        [    0, 19127],\n",
       "        [    0, 18986],\n",
       "        [    0, 19499],\n",
       "        [    0, 14687],\n",
       "        [    0, 18672],\n",
       "        [    0, 17160],\n",
       "        [    0, 17742],\n",
       "        [    0, 14697],\n",
       "        [    0, 16144],\n",
       "        [    0, 16943],\n",
       "        [    0, 16872],\n",
       "        [    0, 17796],\n",
       "        [    0, 14762],\n",
       "        [    0, 16118],\n",
       "        [    0, 17807],\n",
       "        [    0, 14954],\n",
       "        [    0, 18233],\n",
       "        [    0, 14293],\n",
       "        [    0, 18573],\n",
       "        [    0, 16794],\n",
       "        [    0, 15790]])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_test_set(itemsnoninteracted:list, gt_test_interactions: np.ndarray) -> list:\n",
    "    #max_users, max_items = dims # number users (943), number items (2625)\n",
    "    test_set = []\n",
    "    for pair, negatives in tqdm(zip(gt_test_interactions, itemsnoninteracted), desc=\"BUILDING TEST SET...\"):\n",
    "        # APPEND TEST SETS FOR SINGLE USER\n",
    "        negatives = np.delete(negatives, np.where(negatives == pair[1]))\n",
    "        single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
    "        single_user_test_set[:, 1][1:] = negatives\n",
    "        test_set.append(single_user_test_set.copy()) # siempre tendremos 1 positivo y el resto negativos\n",
    "        break\n",
    "    return test_set\n",
    "\n",
    "# test_x = build_test_set(items2compute, test_x)\n",
    "# test_x[0]\n",
    "build_test_set(items2compute, test_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Factorization Machines model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM_operation(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 reduce_sum: bool=True) -> None:\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        # square_of_sum = np.sum(x, dim=1) ** 2 # ...\n",
    "        # sum_of_square = np.sum(x ** 2, dim=1) # ...\n",
    "        \n",
    "        square_of_sum = torch.pow(torch.sum(x, dim=1),2)\n",
    "        sum_of_square = torch.sum(torch.pow(x,2), dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 field_dims: list,\n",
    "                 embed_dim: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(len(field_dims), 1)\n",
    "        self.embedding = torch.nn.Embedding(field_dims[-1], embed_dim)\n",
    "        self.fm = FM_operation(reduce_sum=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, interaction_pairs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        out = self.linear(interaction_pairs.float()) + self.fm(self.embedding(interaction_pairs))\n",
    "        return out.squeeze(1)\n",
    "        \n",
    "    def predict(self, \n",
    "                interactions: np.ndarray,\n",
    "                device: torch.device) -> torch.Tensor:\n",
    "        # return the score, inputs are numpy arrays, outputs are tensors\n",
    "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device) #, dtype=torch.long)\n",
    "        output_scores = self.forward(test_interactions)\n",
    "        return output_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    criterion: torch.nn.functional,\n",
    "                    device: torch.device) -> float:\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "\n",
    "    for i, (interactions, targets) in enumerate(data_loader):\n",
    "        interactions = interactions.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        predictions = model(interactions)\n",
    "    \n",
    "        loss = criterion(predictions, targets.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    return mean(total_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getHitRatio(recommend_list: list,\n",
    "                gt_item: int) -> bool:\n",
    "    if gt_item in recommend_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getNDCG(recommend_list: list,\n",
    "            gt_item: int) -> float:\n",
    "    idx = np.where(recommend_list == gt_item)[0]\n",
    "    if len(idx) > 0:\n",
    "        return math.log(2)/math.log(idx+2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module,\n",
    "         test_x: np.ndarray,\n",
    "         device: torch.device,\n",
    "         topk: int=10) -> Tuple[float, float]:\n",
    "    # Test the HR and NDCG for the model @topK\n",
    "    model.eval()\n",
    "\n",
    "    HR, NDCG = [], []\n",
    "    for user_test in test_x:\n",
    "        gt_item = user_test[0][1]\n",
    "        predictions = model.predict(user_test, device)\n",
    "        _, indices = torch.topk(predictions, topk)\n",
    "        recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
    "\n",
    "        HR.append(getHitRatio(recommend_list, gt_item))\n",
    "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
    "    return mean(HR), mean(NDCG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE\n",
    "## Defining the model, the loss and the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = train_dataset.dims\n",
    "model = FactorizationMachineModel(dims, hparams['hidden_size']).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=hparams['learning_rate'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: list) -> None:\n",
    "        super(RandomModel, self).__init__()\n",
    "        \"\"\"\n",
    "        Simple random based recommender system\n",
    "        \"\"\"\n",
    "        self.all_items = list(range(dims[0], dims[1]))\n",
    "\n",
    "    def forward(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def predict(self,\n",
    "                interactions: np.ndarray,\n",
    "                device=None) -> torch.Tensor:\n",
    "        return torch.FloatTensor(random.sample(self.all_items, len(interactions)))\n",
    "\n",
    "rnd_model = RandomModel(dims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      "training loss = 309.8556 | Eval: HR@10 = 0.0395, NDCG@10 = 0.0181 \n",
      "\n",
      "\n",
      "epoch 1:\n",
      "training loss = 0.5132 | Eval: HR@10 = 0.1502, NDCG@10 = 0.0949 \n",
      "\n",
      "\n",
      "epoch 2:\n",
      "training loss = 0.5054 | Eval: HR@10 = 0.1086, NDCG@10 = 0.0683 \n",
      "\n",
      "\n",
      "epoch 3:\n",
      "training loss = 0.4907 | Eval: HR@10 = 0.1318, NDCG@10 = 0.0809 \n",
      "\n",
      "\n",
      "epoch 4:\n",
      "training loss = 0.4683 | Eval: HR@10 = 0.1715, NDCG@10 = 0.1036 \n",
      "\n",
      "\n",
      "epoch 5:\n",
      "training loss = 0.4477 | Eval: HR@10 = 0.1937, NDCG@10 = 0.1162 \n",
      "\n",
      "\n",
      "epoch 6:\n",
      "training loss = 0.4291 | Eval: HR@10 = 0.2109, NDCG@10 = 0.1257 \n",
      "\n",
      "\n",
      "epoch 7:\n",
      "training loss = 0.4143 | Eval: HR@10 = 0.2266, NDCG@10 = 0.1335 \n",
      "\n",
      "\n",
      "epoch 8:\n",
      "training loss = 0.4038 | Eval: HR@10 = 0.2379, NDCG@10 = 0.1396 \n",
      "\n",
      "\n",
      "epoch 9:\n",
      "training loss = 0.3973 | Eval: HR@10 = 0.2465, NDCG@10 = 0.1442 \n",
      "\n",
      "\n",
      "epoch 10:\n",
      "training loss = 0.3923 | Eval: HR@10 = 0.2525, NDCG@10 = 0.1478 \n",
      "\n",
      "\n",
      "epoch 11:\n",
      "training loss = 0.3867 | Eval: HR@10 = 0.2494, NDCG@10 = 0.1461 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO EPOCHS NOW\n",
    "topk = 10\n",
    "for epoch_i in range(hparams['num_epochs']):\n",
    "    #data_loader.dataset.negative_sampling()\n",
    "    train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
    "    hr, ndcg = test(model, test_x, device, topk=topk)\n",
    "\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    print('\\n')\n",
    " \n",
    "    tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
    "    tb_fm.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_fm.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "    hr, ndcg = test(rnd_model, test_x, device, topk=topk)\n",
    "    tb_rnd.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_rnd.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"kill\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.getcwd() / Path(\"4_Modelling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir run_tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977d1ca4f86cf6796d22eaa6050e2f72f1e03740aa8986f601710bf090b5ff9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
