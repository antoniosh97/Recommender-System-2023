{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#====================== Import de librerias =====================#\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import wget\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from tqdm import tqdm, trange\n",
    "from IPython import embed\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "import webbrowser\n",
    "\n",
    "sampling_method = os.listdir()[4].split(\".\")[-2][3:].split(\"_\")[-1]\n",
    "logfile = \"project.log\"\n",
    "old_path = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "execution_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "logs_base_dir = \"runs_\"+sampling_method\n",
    "os.environ[\"run_tensorboard\"] = logs_base_dir\n",
    "\n",
    "os.makedirs(f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}', exist_ok=True)\n",
    "tb_fm = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_FM/')\n",
    "tb_rnd = SummaryWriter(log_dir=f'{execution_path}/{\"4_Modelling\"}/{logs_base_dir}/{logs_base_dir}_RANDOM/')\n",
    "\n",
    "def save_data_configuration(text):\n",
    "    save_data_dir = \"data_config_\" + sampling_method +  \".txt\"\n",
    "    path = f'{execution_path}/{\"4_Modelling\"}/{save_data_dir}'\n",
    "    with open(path, \"a\") as data_file:\n",
    "        data_file.write(text+\"\\n\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some hyper-parameters\n",
    "hparams = {\n",
    "    'batch_size':64,\n",
    "    'num_epochs':12,\n",
    "    'hidden_size': 32,\n",
    "    'learning_rate':1e-4,\n",
    "}\n",
    "\n",
    "# we select to work on GPU if it is available in the machine, otherwise\n",
    "# will run on CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Definicion de valores de configuracion ============#\n",
    "\n",
    "min_reviews, min_usuarios = [6,6]\n",
    "col_names = {\"col_id_reviewer\": \"reviewerID\",\n",
    "             \"col_id_product\": \"asin\",\n",
    "             \"col_unix_time\": \"unixReviewTime\",\n",
    "             \"col_rating\": \"overall\",\n",
    "             \"col_timestamp\": \"timestamp\",\n",
    "             \"col_year\": \"year\"}\n",
    "\n",
    "csv_filename = execution_path/Path(\"3_DataPreparation/interactions_minR{}_minU{}.csv\".format(min_reviews,min_usuarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1477785600</td>\n",
       "      <td>2016-10-30 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10612</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1467244800</td>\n",
       "      <td>2016-06-30 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1454716800</td>\n",
       "      <td>2016-02-06 01:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1434844800</td>\n",
       "      <td>2015-06-21 02:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2523</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1420329600</td>\n",
       "      <td>2015-01-04 01:00:00</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asin  reviewerID  overall  unixReviewTime            timestamp  year\n",
       "0     0        9132      5.0      1477785600  2016-10-30 02:00:00  1970\n",
       "1     0       10612      5.0      1467244800  2016-06-30 02:00:00  1970\n",
       "2     0         257      1.0      1454716800  2016-02-06 01:00:00  1970\n",
       "3     0        4425      5.0      1434844800  2015-06-21 02:00:00  1970\n",
       "4     0        2523      4.0      1420329600  2015-01-04 01:00:00  1970"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin               6178\n",
       "reviewerID        14138\n",
       "overall               5\n",
       "unixReviewTime     3622\n",
       "timestamp          3622\n",
       "year                  1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_configuration(str(df.nunique()))\n",
    "df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset (TLOO strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data: np.ndarray,\n",
    "                     n_users: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Split and remove timestamp\n",
    "    train_x, test_x = [], []\n",
    "    for u in trange(n_users, desc='spliting train/test and removing timestamp...'):\n",
    "        user_data = data[data[:, 0] == u]\n",
    "        sorted_data = user_data[user_data[:, -1].argsort()]\n",
    "        if len(sorted_data) == 1:\n",
    "            train_x.append(sorted_data[0][:-1])\n",
    "        else:\n",
    "            train_x.append(sorted_data[:-1][:, :-1])\n",
    "            test_x.append(sorted_data[-1][:-1])\n",
    "    return np.vstack(train_x), np.stack(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      9132,          0, 1477785600],\n",
       "       [     10612,          0, 1467244800],\n",
       "       [       257,          0, 1454716800],\n",
       "       ...,\n",
       "       [      9051,       6177, 1530144000],\n",
       "       [      3412,       6177, 1527465600],\n",
       "       [      9805,       6177, 1527206400]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[*col_names.values()][:3]].astype('int32').to_numpy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim of users: 14138\n",
      "Dim of items: 20316\n",
      "Dims of unixtime: 1538006401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[      9132,      14138, 1477785600],\n",
       "       [     10612,      14138, 1467244800],\n",
       "       [       257,      14138, 1454716800],\n",
       "       ...,\n",
       "       [      9051,      20315, 1530144000],\n",
       "       [      3412,      20315, 1527465600],\n",
       "       [      9805,      20315, 1527206400]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_dims=0\n",
    "for i in range(data.shape[1] - 1):  # do not affect to timestamp\n",
    "    # MAKE IT START BY 0\n",
    "    data[:, i] -= np.min(data[:, i])\n",
    "    # RE-INDEX\n",
    "    data[:, i] += add_dims\n",
    "    add_dims = np.max(data[:, i]) + 1\n",
    "dims = np.max(data, axis=0) + 1\n",
    "print(\"Dim of users: {}\\nDim of items: {}\\nDims of unixtime: {}\".format(dims[0], dims[1], dims[2]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting train/test and removing timestamp...: 100%|██████████| 14138/14138 [00:05<00:00, 2818.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248],\n",
       "       [    0, 19249],\n",
       "       [    0, 14823],\n",
       "       ...,\n",
       "       [14137, 14159],\n",
       "       [14137, 18245],\n",
       "       [14137, 18904]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x = split_train_test(data, dims[0])\n",
    "train_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dims: [14138 20316]\n",
      "New train_x:\n",
      " [[    0 19248]\n",
      " [    0 19249]\n",
      " [    0 14823]\n",
      " ...\n",
      " [14137 14159]\n",
      " [14137 18245]\n",
      " [14137 18904]]\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x[:, :2]\n",
    "dims = dims[:2]\n",
    "print(\"New dims:\",dims)\n",
    "print(\"New train_x:\\n\",train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adj_mx(n_feat:int, data:np.ndarray) -> sp.dok_matrix :\n",
    "    train_mat = sp.dok_matrix((n_feat, n_feat), dtype=np.float32)\n",
    "    for x in tqdm(data, desc=f\"BUILDING ADJACENCY MATRIX...\"):\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "        train_mat[x[1], x[0]] = 1.0\n",
    "        # IDEA: We treat features that are not user or item differently because we do not consider\n",
    "        #  interactions between contexts\n",
    "        if data.shape[1] > 2:\n",
    "            for idx in range(len(x[2:])):\n",
    "                train_mat[x[0], x[2 + idx]] = 1.0\n",
    "                train_mat[x[1], x[2 + idx]] = 1.0\n",
    "                train_mat[x[2 + idx], x[0]] = 1.0\n",
    "                train_mat[x[2 + idx], x[1]] = 1.0\n",
    "    return train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ng_sample(data: np.ndarray, dims: list, num_ng:int=4) -> Tuple[np.ndarray, sp.dok_matrix]:\n",
    "    rating_mat = build_adj_mx(dims[-1], data)\n",
    "    interactions = []\n",
    "    min_item, max_item = dims[0], dims[1]\n",
    "    for num, x in tqdm(enumerate(data), desc='perform negative sampling...'):\n",
    "        interactions.append(np.append(x, 1))\n",
    "        for t in range(num_ng):\n",
    "            j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
    "            print([[x[0], j], x[2:], [0]])\n",
    "            borrar = np.vstack(interactions)\n",
    "            print(borrar[:,:2])\n",
    "            # print(interactions[j])\n",
    "            print([x[0], j])\n",
    "            print( [x[0], j] in borrar[:,:2])\n",
    "            # print(j == int(x[1]))\n",
    "            # print((x[0], j) in rating_mat)\n",
    "            while (x[0], j) in rating_mat or j == int(x[1]) :#or [x[0], j] in interactions[:,:2]:\n",
    "                j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "                \n",
    "            interactions.append(np.concatenate([[x[0], j], x[2:], [0]]))\n",
    "        break\n",
    "    return np.vstack(interactions), rating_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if element is in list\n",
    "\"\"\"\n",
    "def isInList(element, list):\n",
    "    for negative in list:\n",
    "        if (set(element) == set(element) & set(negative)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def ng_sample(data: np.ndarray, dims: list, num_ng:int=4) -> Tuple[np.ndarray, sp.dok_matrix]:\n",
    "    rating_mat = build_adj_mx(dims[-1], data)\n",
    "    interactions = []\n",
    "    user = -1\n",
    "    min_item, max_item = dims[0], dims[1]\n",
    "    for num, x in tqdm(enumerate(data), desc='perform negative sampling...'):\n",
    "        interactions.append(x) # Añadimos de uno en uno, x, que es positivo (ya viene el rating '1')  \n",
    "        if user != x[0]: # en cada cambio de usuario\n",
    "            userItem = [] \n",
    "            user = x[0]    \n",
    "        userItem.append(x)\n",
    "        for t in range(num_ng): # vamos añadir k negativos random\n",
    "            j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
    "            #       also exclude items duplicated for the same user\n",
    "            while ((x[0], j) in rating_mat or j == int(x[1]) or isInList(np.concatenate([[x[0], j], x[3:], [0]]), userItem)):\n",
    "                j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
    "            interactions.append(np.concatenate([[x[0], j], x[3:], [0]]))\n",
    "    return np.vstack(interactions), rating_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248],\n",
       "       [    0, 19249],\n",
       "       [    0, 14823],\n",
       "       ...,\n",
       "       [14137, 14159],\n",
       "       [14137, 18245],\n",
       "       [14137, 18904]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup=pd.DataFrame(train_x)\n",
    "dup[dup.duplicated()]\n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING ADJACENCY MATRIX...: 100%|██████████| 123226/123226 [00:03<00:00, 36537.54it/s]\n",
      "perform negative sampling...: 123226it [00:25, 4904.32it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\brend\\AppData\\Local\\Temp\\ipykernel_18616\\2540131526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mng_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\brend\\AppData\\Local\\Temp\\ipykernel_18616\\1052845904.py\u001b[0m in \u001b[0;36mng_sample\u001b[1;34m(data, dims, num_ng)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_item\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#if not pop else random.sample(items_to_sample, 1)[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0minteractions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_mat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brend\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 3"
     ]
    }
   ],
   "source": [
    "a, b = ng_sample(train_x, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING ADJACENCY MATRIX...: 100%|██████████| 123226/123226 [00:03<00:00, 38693.11it/s]\n",
      "perform negative sampling...: 0it [00:00, ?it/s]C:\\Users\\brend\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if sys.path[0] == \"\":\n",
      "C:\\Users\\brend\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if sys.path[0] == \"\":\n",
      "perform negative sampling...: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 19166], array([], dtype=int32), [0]]\n",
      "[array([    0, 19248,     1])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\brend\\AppData\\Local\\Temp\\ipykernel_11672\\3345323482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mng_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dimensions matrix:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(\"\\nRating matrix:\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# rating_mat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\brend\\AppData\\Local\\Temp\\ipykernel_11672\\3139753973.py\u001b[0m in \u001b[0;36mng_sample\u001b[1;34m(data, dims, num_ng)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minteractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrating_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "train_x, rating_mat = ng_sample(train_x, dims)\n",
    "print(\"Dimensions matrix:\\n\",dims)\n",
    "# print(\"\\nRating matrix:\")\n",
    "# rating_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 16654]\n",
    "a = np.array([[    0, 19248],\n",
    " [    0, 18304]])\n",
    "\n",
    "\"True\" in pd.DataFrame(a).duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0      1  2\n",
      "709        11  16982  0\n",
      "1279       25  20183  0\n",
      "1306       25  16952  0\n",
      "1599       30  14156  0\n",
      "1621       31  15548  0\n",
      "...       ...    ... ..\n",
      "615247  14118  14408  0\n",
      "615427  14122  19335  0\n",
      "615762  14127  18717  0\n",
      "615819  14128  14632  0\n",
      "615857  14128  16571  0\n",
      "\n",
      "[2066 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "dup=pd.DataFrame(train_x)\n",
    "print(dup[dup.duplicated()])\n",
    "# train_x\n",
    "# t = [0, 18053, 0]\n",
    "# a = np.array([[    0, 19248,     1],\n",
    "#        [    0, 18053,     0],\n",
    "#        [    0, 15058,     0],\n",
    "#        [14137, 16599,     0],\n",
    "#        [14137, 17383,     0],\n",
    "#        [14137, 15885,     0]])\n",
    "# t in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>25</td>\n",
       "      <td>20303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>25</td>\n",
       "      <td>20280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>25</td>\n",
       "      <td>20183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>25</td>\n",
       "      <td>20183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>25</td>\n",
       "      <td>20078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>25</td>\n",
       "      <td>14306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>25</td>\n",
       "      <td>14291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>25</td>\n",
       "      <td>14255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>25</td>\n",
       "      <td>14176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>25</td>\n",
       "      <td>14151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1  2\n",
       "1282  25  20303  0\n",
       "1295  25  20280  1\n",
       "1279  25  20183  0\n",
       "1247  25  20183  0\n",
       "1319  25  20078  0\n",
       "...   ..    ... ..\n",
       "1312  25  14306  0\n",
       "1292  25  14291  0\n",
       "1272  25  14255  0\n",
       "1267  25  14176  0\n",
       "1264  25  14151  0\n",
       "\n",
       "[85 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup[dup[0]==25].sort_values(by=[1], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6178"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims[-1]-dims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000597112191656141\n",
      "0.9994028878083439\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "## Evaristo\n",
    "#### number of ones\n",
    "print(np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "### number of zeros\n",
    "print(1 - np.count_nonzero(rating_mat.toarray())/(dims[-1]*dims[-1]))\n",
    "\n",
    "# ## Brenda\n",
    "# #### Who sparse is the matrix??\n",
    "# print(1 - rating_mat.shape[0] / rating_mat.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 19248,     1],\n",
       "       [    0, 16988,     0],\n",
       "       [    0, 15331,     0],\n",
       "       [    0, 16732,     0],\n",
       "       [    0, 18032,     0],\n",
       "       [    0, 19249,     1],\n",
       "       [    0, 18445,     0],\n",
       "       [    0, 14442,     0],\n",
       "       [    0, 17291,     0],\n",
       "       [    0, 16161,     0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointData(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 dims: list) -> None:\n",
    "        \"\"\"\n",
    "        Dataset formatter adapted point-wise algorithms\n",
    "        Parameters\n",
    "        \"\"\"\n",
    "        super(PointData, self).__init__()\n",
    "        self.interactions = data\n",
    "        self.dims = dims\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.interactions)\n",
    "        \n",
    "    def __getitem__(self, \n",
    "                    index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return the pairs user-item and the target.\n",
    "        \"\"\"\n",
    "        return self.interactions[index][:-1], self.interactions[index][-1]\n",
    "\n",
    "train_dataset = PointData(train_x, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0, 19248]), 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the test set for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 17249],\n",
       "       [    1, 18015],\n",
       "       [    2, 14196],\n",
       "       ...,\n",
       "       [14135, 19938],\n",
       "       [14136, 20214],\n",
       "       [14137, 15542]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20316, 20316)\n",
      "rating_mat contains log2(rating_mat.shape[0]) = 15 bits\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(rating_mat.shape)\n",
    "bits = math.ceil(math.log(rating_mat.shape[0],2))\n",
    "print(\"rating_mat contains log2(rating_mat.shape[0]) = {} bits\".format(bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_no_interactions_stratified_users(train_x: np.ndarray, test_x: np.ndarray, dims_usuarios_productos: Tuple[int, int],  num_samples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Esta funcion se encarga de crear de manera eficiente un dataset que contenga las interacciones usuario-producto en test que no se hayan producido.\n",
    "    \n",
    "    Argumentos:\n",
    "        train_x (np.ndarray): matriz de entrenamiento con las interacciones usuario-producto previas\n",
    "        test_x (np.ndarray): matriz de prueba con las interacciones usuario-producto previas\n",
    "        dims_usuarios_productos (Tuple[int, int]): rango de productos y usuarios disponibles\n",
    "    \n",
    "    Retorno:\n",
    "        np.ndarray: una matriz con todas las interacciones usuario-producto en test que no se hayan producido\n",
    "    \"\"\"\n",
    "    save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: Stratified Sampling By Users  \"+\"#\"*4)\n",
    "\n",
    "    \n",
    "    seed = 2   # get same results\n",
    "    random.seed(a = seed)\n",
    "\n",
    "    usuarios_test = np.unique(test_x[:, 0]) # Identificamos los usuarios presentes en la prueba\n",
    "    total_productos = range(dims_usuarios_productos[0]-1, dims_usuarios_productos[1]) # Identificamos el rango de productos disponibles\n",
    "    zero_positions = np.zeros((num_samples * len(usuarios_test), 2)).astype(int)\n",
    "    start_index = 0\n",
    "    \n",
    "    # Recorremos cada usuario presente en la prueba\n",
    "    for user in tqdm(usuarios_test):\n",
    "        # Identificamos los productos en los que el usuario ha interactuado previamente en entrenamiento\n",
    "        strata_items_train = np.unique(train_x[train_x[:, 0] == user][:, 1])\n",
    "        strata_items_test = np.unique(test_x[test_x[:, 0] == user][:, 1])\n",
    "        # Seleccionamos al azar 199 productos con los que el usuario no ha interactuado previamente\n",
    "        random_selection_items = random.choices(list(set(total_productos) - set(strata_items_test) - set(strata_items_train)), k=num_samples)\n",
    "        # Creamos una lista de interacciones usuario-producto para este usuario\n",
    "        # lista_por_usuario = [[usuario, x] for x in productos_a_machear]\n",
    "        # We must avoid using for loops!\n",
    "\n",
    "        zero_positions[start_index:start_index + num_samples, 0] = user\n",
    "        zero_positions[start_index:start_index + num_samples, 1] = random_selection_items\n",
    "        start_index += num_samples\n",
    "        # lista_por_usuario = np.vstack([(np.ones(len(productos_a_machear))*usuario).astype(int), productos_a_machear]).T\n",
    "        \n",
    "        # Si es el primer usuario, inicializamos una matriz con sus interacciones\n",
    "        # if usuario == 0:\n",
    "        #     zero_positions = lista_por_usuario\n",
    "        # # Si no es el primer usuario, concatenamos sus interacciones a la matriz existente\n",
    "        # else:\n",
    "        #     zero_positions = np.concatenate((zero_positions, lista_por_usuario), axis=0)\n",
    "    # [00:59<00:00, 236.39it/s]\n",
    "    return zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zero_positions_mode(mode, rating_mat, train_x, test_x, dims):\n",
    "   \n",
    "#     if mode == 0:\n",
    "#         save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data  \"+\"#\"*4)\n",
    "#         return np.asarray(np.where(rating_mat.A==0)).T\n",
    "#     elif mode == 1:\n",
    "#         zero_true_matrix = np.where(rating_mat.A==0)\n",
    "#         save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: all data separated by rows  \"+\"#\"*4)\n",
    "#         return np.asarray([zero_true_matrix[0],zero_true_matrix[1]]).T\n",
    "#     else:\n",
    "#         save_data_configuration(\"\\n\"+\"#\"*4+\"  zero_positions: random sampling  \"+\"#\"*4)\n",
    "#         zp = create_test_no_interactions(train_x, test_x, dims,  num_samples=199)\n",
    "#         return zp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zp = zero_positions_mode(2, rating_mat, train_x, test_x, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14138/14138 [00:32<00:00, 437.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2813462, 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 16115],\n",
       "       [    0, 16065],\n",
       "       [    0, 16733],\n",
       "       ...,\n",
       "       [14137, 16332],\n",
       "       [14137, 15660],\n",
       "       [14137, 14457]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero_positions = np.asarray(np.where(rating_mat.A==0)).T\n",
    "zero_positions = create_test_no_interactions_stratified_users(train_x, test_x, dims,  num_samples=199)\n",
    "print(save_data_configuration(str(zero_positions.shape)+\"\\n\"))\n",
    "zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14138/14138 [01:59<00:00, 117.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([16115, 16065, 16733, 16914, 15372, 14755, 14338, 18290, 20123,\n",
       "       20128, 19971, 17366, 19049, 18819, 14674, 16352, 16075, 19747,\n",
       "       19135, 18044, 16605, 16553, 19258, 18352, 18735, 15722, 19635,\n",
       "       19846, 17842, 16531, 18392, 17230, 19539, 16376, 14367, 17510,\n",
       "       15733, 15133, 14746, 15812, 14922, 15089, 18568, 16265, 16150,\n",
       "       17383, 14867, 14624, 19235, 19662, 19412, 15924, 19480, 15348,\n",
       "       18568, 15666, 15770, 19233, 19890, 15896, 14679, 19392, 17755,\n",
       "       18389, 14526, 17412, 15820, 18044, 15842, 18298, 16123, 14566,\n",
       "       19501, 19586, 14224, 20013, 18312, 17670, 19550, 15982, 20228,\n",
       "       16852, 15280, 14693, 15818, 17569, 14810, 16746, 14233, 18074,\n",
       "       17784, 15621, 17045, 19614, 15487, 17896, 17686, 15653, 18999,\n",
       "       14635, 16580, 18622, 17447, 14357, 16902, 16106, 16541, 14714,\n",
       "       16514, 17963, 15236, 17358, 17522, 14475, 18769, 16649, 16322,\n",
       "       17323, 16607, 18508, 20180, 14795, 17087, 18466, 16573, 19158,\n",
       "       14942, 14779, 15784, 14877, 15539, 14561, 19306, 17778, 14282,\n",
       "       18339, 17020, 19153, 15617, 17175, 19994, 18816, 19568, 17276,\n",
       "       16137, 17984, 20124, 18980, 16496, 19831, 17256, 16734, 16591,\n",
       "       17383, 16981, 20302, 19527, 16281, 15982, 16351, 17821, 19134,\n",
       "       17931, 20034, 20233, 15154, 14589, 17968, 19000, 19637, 16415,\n",
       "       16603, 18913, 17075, 14679, 17872, 17005, 17509, 17815, 17728,\n",
       "       19603, 19255, 18299, 14165, 17697, 15812, 16157, 14711, 19067,\n",
       "       19548, 19971, 16699, 18970, 19630, 17506, 16968, 15170, 18646,\n",
       "       19594])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta parte del código se usaba porque el zero positions venia de la rating matrix la cual tenia informacion no útil como (user,user) o (item,item), por eso nos quedábamos con items mayores a dims[0]\n",
    "Ya que si recordamos, la matrix tenia size: (users + items)\n",
    "rango items iniciales = 0 ... 6177\n",
    "rango items actuales  = 14138 ... 20135\n",
    "\"\"\"\n",
    "\n",
    "items2compute = []\n",
    "for user in trange(dims[0]):\n",
    "    aux = zero_positions[zero_positions[:, 0] == user][:, 1]\n",
    "    items2compute.append(aux[aux >= dims[0]])\n",
    "items2compute[0]\n",
    "\n",
    "# MUY RÁPIDO !!!!!\n",
    "# items2compute = []\n",
    "# start = 0\n",
    "# num_samples = 199\n",
    "# for user in trange(dims[0]):\n",
    "#     aux = zero_positions[start:start+num_samples,1]\n",
    "#     start += num_samples \n",
    "#     items2compute.append(aux)\n",
    "# items2compute[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING TEST SET...: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_test_set(itemsnoninteracted:list, gt_test_interactions: np.ndarray) -> list:\n",
    "    #max_users, max_items = dims # number users (943), number items (2625)\n",
    "    test_set = []\n",
    "    for pair, negatives in tqdm(zip(gt_test_interactions, itemsnoninteracted), desc=\"BUILDING TEST SET...\"):\n",
    "        # APPEND TEST SETS FOR SINGLE USER\n",
    "        negatives = np.delete(negatives, np.where(negatives == pair[1]))\n",
    "        single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
    "        single_user_test_set[:, 1][1:] = negatives\n",
    "        test_set.append(single_user_test_set.copy()) # siempre tendremos 1 positivo y el resto negativos\n",
    "        break\n",
    "    return test_set\n",
    "\n",
    "test_x = build_test_set(items2compute, test_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Factorization Machines model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[    0, 17249],\n",
       "        [    0, 16115],\n",
       "        [    0, 16065],\n",
       "        [    0, 16733],\n",
       "        [    0, 16914],\n",
       "        [    0, 15372],\n",
       "        [    0, 14755],\n",
       "        [    0, 14338],\n",
       "        [    0, 18290],\n",
       "        [    0, 20123],\n",
       "        [    0, 20128],\n",
       "        [    0, 19971],\n",
       "        [    0, 17366],\n",
       "        [    0, 19049],\n",
       "        [    0, 18819],\n",
       "        [    0, 14674],\n",
       "        [    0, 16352],\n",
       "        [    0, 16075],\n",
       "        [    0, 19747],\n",
       "        [    0, 19135],\n",
       "        [    0, 18044],\n",
       "        [    0, 16605],\n",
       "        [    0, 16553],\n",
       "        [    0, 19258],\n",
       "        [    0, 18352],\n",
       "        [    0, 18735],\n",
       "        [    0, 15722],\n",
       "        [    0, 19635],\n",
       "        [    0, 19846],\n",
       "        [    0, 17842],\n",
       "        [    0, 16531],\n",
       "        [    0, 18392],\n",
       "        [    0, 17230],\n",
       "        [    0, 19539],\n",
       "        [    0, 16376],\n",
       "        [    0, 14367],\n",
       "        [    0, 17510],\n",
       "        [    0, 15733],\n",
       "        [    0, 15133],\n",
       "        [    0, 14746],\n",
       "        [    0, 15812],\n",
       "        [    0, 14922],\n",
       "        [    0, 15089],\n",
       "        [    0, 18568],\n",
       "        [    0, 16265],\n",
       "        [    0, 16150],\n",
       "        [    0, 17383],\n",
       "        [    0, 14867],\n",
       "        [    0, 14624],\n",
       "        [    0, 19235],\n",
       "        [    0, 19662],\n",
       "        [    0, 19412],\n",
       "        [    0, 15924],\n",
       "        [    0, 19480],\n",
       "        [    0, 15348],\n",
       "        [    0, 18568],\n",
       "        [    0, 15666],\n",
       "        [    0, 15770],\n",
       "        [    0, 19233],\n",
       "        [    0, 19890],\n",
       "        [    0, 15896],\n",
       "        [    0, 14679],\n",
       "        [    0, 19392],\n",
       "        [    0, 17755],\n",
       "        [    0, 18389],\n",
       "        [    0, 14526],\n",
       "        [    0, 17412],\n",
       "        [    0, 15820],\n",
       "        [    0, 18044],\n",
       "        [    0, 15842],\n",
       "        [    0, 18298],\n",
       "        [    0, 16123],\n",
       "        [    0, 14566],\n",
       "        [    0, 19501],\n",
       "        [    0, 19586],\n",
       "        [    0, 14224],\n",
       "        [    0, 20013],\n",
       "        [    0, 18312],\n",
       "        [    0, 17670],\n",
       "        [    0, 19550],\n",
       "        [    0, 15982],\n",
       "        [    0, 20228],\n",
       "        [    0, 16852],\n",
       "        [    0, 15280],\n",
       "        [    0, 14693],\n",
       "        [    0, 15818],\n",
       "        [    0, 17569],\n",
       "        [    0, 14810],\n",
       "        [    0, 16746],\n",
       "        [    0, 14233],\n",
       "        [    0, 18074],\n",
       "        [    0, 17784],\n",
       "        [    0, 15621],\n",
       "        [    0, 17045],\n",
       "        [    0, 19614],\n",
       "        [    0, 15487],\n",
       "        [    0, 17896],\n",
       "        [    0, 17686],\n",
       "        [    0, 15653],\n",
       "        [    0, 18999],\n",
       "        [    0, 14635],\n",
       "        [    0, 16580],\n",
       "        [    0, 18622],\n",
       "        [    0, 17447],\n",
       "        [    0, 14357],\n",
       "        [    0, 16902],\n",
       "        [    0, 16106],\n",
       "        [    0, 16541],\n",
       "        [    0, 14714],\n",
       "        [    0, 16514],\n",
       "        [    0, 17963],\n",
       "        [    0, 15236],\n",
       "        [    0, 17358],\n",
       "        [    0, 17522],\n",
       "        [    0, 14475],\n",
       "        [    0, 18769],\n",
       "        [    0, 16649],\n",
       "        [    0, 16322],\n",
       "        [    0, 17323],\n",
       "        [    0, 16607],\n",
       "        [    0, 18508],\n",
       "        [    0, 20180],\n",
       "        [    0, 14795],\n",
       "        [    0, 17087],\n",
       "        [    0, 18466],\n",
       "        [    0, 16573],\n",
       "        [    0, 19158],\n",
       "        [    0, 14942],\n",
       "        [    0, 14779],\n",
       "        [    0, 15784],\n",
       "        [    0, 14877],\n",
       "        [    0, 15539],\n",
       "        [    0, 14561],\n",
       "        [    0, 19306],\n",
       "        [    0, 17778],\n",
       "        [    0, 14282],\n",
       "        [    0, 18339],\n",
       "        [    0, 17020],\n",
       "        [    0, 19153],\n",
       "        [    0, 15617],\n",
       "        [    0, 17175],\n",
       "        [    0, 19994],\n",
       "        [    0, 18816],\n",
       "        [    0, 19568],\n",
       "        [    0, 17276],\n",
       "        [    0, 16137],\n",
       "        [    0, 17984],\n",
       "        [    0, 20124],\n",
       "        [    0, 18980],\n",
       "        [    0, 16496],\n",
       "        [    0, 19831],\n",
       "        [    0, 17256],\n",
       "        [    0, 16734],\n",
       "        [    0, 16591],\n",
       "        [    0, 17383],\n",
       "        [    0, 16981],\n",
       "        [    0, 20302],\n",
       "        [    0, 19527],\n",
       "        [    0, 16281],\n",
       "        [    0, 15982],\n",
       "        [    0, 16351],\n",
       "        [    0, 17821],\n",
       "        [    0, 19134],\n",
       "        [    0, 17931],\n",
       "        [    0, 20034],\n",
       "        [    0, 20233],\n",
       "        [    0, 15154],\n",
       "        [    0, 14589],\n",
       "        [    0, 17968],\n",
       "        [    0, 19000],\n",
       "        [    0, 19637],\n",
       "        [    0, 16415],\n",
       "        [    0, 16603],\n",
       "        [    0, 18913],\n",
       "        [    0, 17075],\n",
       "        [    0, 14679],\n",
       "        [    0, 17872],\n",
       "        [    0, 17005],\n",
       "        [    0, 17509],\n",
       "        [    0, 17815],\n",
       "        [    0, 17728],\n",
       "        [    0, 19603],\n",
       "        [    0, 19255],\n",
       "        [    0, 18299],\n",
       "        [    0, 14165],\n",
       "        [    0, 17697],\n",
       "        [    0, 15812],\n",
       "        [    0, 16157],\n",
       "        [    0, 14711],\n",
       "        [    0, 19067],\n",
       "        [    0, 19548],\n",
       "        [    0, 19971],\n",
       "        [    0, 16699],\n",
       "        [    0, 18970],\n",
       "        [    0, 19630],\n",
       "        [    0, 17506],\n",
       "        [    0, 16968],\n",
       "        [    0, 15170],\n",
       "        [    0, 18646],\n",
       "        [    0, 19594]])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM_operation(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 reduce_sum: bool=True) -> None:\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        # square_of_sum = np.sum(x, dim=1) ** 2 # ...\n",
    "        # sum_of_square = np.sum(x ** 2, dim=1) # ...\n",
    "        \n",
    "        square_of_sum = torch.pow(torch.sum(x, dim=1),2)\n",
    "        sum_of_square = torch.sum(torch.pow(x,2), dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 field_dims: list,\n",
    "                 embed_dim: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(len(field_dims), 1)\n",
    "        self.embedding = torch.nn.Embedding(field_dims[-1], embed_dim)\n",
    "        self.fm = FM_operation(reduce_sum=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, interaction_pairs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        out = self.linear(interaction_pairs.float()) + self.fm(self.embedding(interaction_pairs))\n",
    "        return out.squeeze(1)\n",
    "        \n",
    "    def predict(self, \n",
    "                interactions: np.ndarray,\n",
    "                device: torch.device) -> torch.Tensor:\n",
    "        # return the score, inputs are numpy arrays, outputs are tensors\n",
    "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device) #, dtype=torch.long)\n",
    "        output_scores = self.forward(test_interactions)\n",
    "        return output_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim,\n",
    "                    data_loader: torch.utils.data.DataLoader,\n",
    "                    criterion: torch.nn.functional,\n",
    "                    device: torch.device) -> float:\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "\n",
    "    for i, (interactions, targets) in enumerate(data_loader):\n",
    "        interactions = interactions.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        predictions = model(interactions)\n",
    "    \n",
    "        loss = criterion(predictions, targets.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    return mean(total_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getHitRatio(recommend_list: list,\n",
    "                gt_item: int) -> bool:\n",
    "    if gt_item in recommend_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getNDCG(recommend_list: list,\n",
    "            gt_item: int) -> float:\n",
    "    idx = np.where(recommend_list == gt_item)[0]\n",
    "    if len(idx) > 0:\n",
    "        return math.log(2)/math.log(idx+2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: torch.nn.Module,\n",
    "         test_x: np.ndarray,\n",
    "         device: torch.device,\n",
    "         topk: int=10) -> Tuple[float, float]:\n",
    "    # Test the HR and NDCG for the model @topK\n",
    "    model.eval()\n",
    "\n",
    "    HR, NDCG = [], []\n",
    "    for user_test in test_x:\n",
    "        gt_item = user_test[0][1]\n",
    "        predictions = model.predict(user_test, device)\n",
    "        _, indices = torch.topk(predictions, topk)\n",
    "        recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
    "\n",
    "        HR.append(getHitRatio(recommend_list, gt_item))\n",
    "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
    "    return mean(HR), mean(NDCG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE\n",
    "## Defining the model, the loss and the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = train_dataset.dims\n",
    "model = FactorizationMachineModel(dims, hparams['hidden_size']).to(device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=hparams['learning_rate'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 dims: list) -> None:\n",
    "        super(RandomModel, self).__init__()\n",
    "        \"\"\"\n",
    "        Simple random based recommender system\n",
    "        \"\"\"\n",
    "        self.all_items = list(range(dims[0], dims[1]))\n",
    "\n",
    "    def forward(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def predict(self,\n",
    "                interactions: np.ndarray,\n",
    "                device=None) -> torch.Tensor:\n",
    "        return torch.FloatTensor(random.sample(self.all_items, len(interactions)))\n",
    "\n",
    "rnd_model = RandomModel(dims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 0:\n",
      "training loss = 61.2017 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 0:\n",
      "training loss = 61.2017 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 1:\n",
      "training loss = 0.5166 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 1:\n",
      "training loss = 0.5166 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 2:\n",
      "training loss = 0.5054 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 2:\n",
      "training loss = 0.5054 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 3:\n",
      "training loss = 0.4869 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 3:\n",
      "training loss = 0.4869 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 4:\n",
      "training loss = 0.4625 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 4:\n",
      "training loss = 0.4625 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 5:\n",
      "training loss = 0.4411 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 5:\n",
      "training loss = 0.4411 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 6:\n",
      "training loss = 0.4247 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 6:\n",
      "training loss = 0.4247 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 7:\n",
      "training loss = 0.4105 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 7:\n",
      "training loss = 0.4105 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 8:\n",
      "training loss = 0.4049 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 8:\n",
      "training loss = 0.4049 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 9:\n",
      "training loss = 0.3975 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 9:\n",
      "training loss = 0.3975 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 10:\n",
      "training loss = 0.3922 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 10:\n",
      "training loss = 0.3922 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: FACTORIZATION MACHINE\n",
      "epoch 11:\n",
      "training loss = 0.3918 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n",
      "MODEL: RANDOM\n",
      "epoch 11:\n",
      "training loss = 0.3918 | Eval: HR@10 = 0.0000, NDCG@10 = 0.0000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Training duration: 1142.6090068817139'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO EPOCHS NOW\n",
    "from datetime import datetime\n",
    "save_data_configuration(datetime.now().strftime(\"%d-%b-%Y  %H:%M\"))\n",
    "time_start = time.time()\n",
    "topk = 10\n",
    "for epoch_i in range(hparams['num_epochs']):\n",
    "    #data_loader.dataset.negative_sampling()\n",
    "    train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
    "    hr, ndcg = test(model, test_x, device, topk=topk)\n",
    "    \n",
    "    print(save_data_configuration(f'MODEL: FACTORIZATION MACHINE'))\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    \n",
    " \n",
    "    tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
    "    tb_fm.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_fm.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "\n",
    "    hr, ndcg = test(rnd_model, test_x, device, topk=topk)\n",
    "    \n",
    "    print(save_data_configuration(f'MODEL: RANDOM'))\n",
    "    print(save_data_configuration(f'epoch {epoch_i}:'))\n",
    "    print(save_data_configuration(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} '))\n",
    "    save_data_configuration(\"_\"*65)\n",
    " \n",
    "    tb_rnd.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "    tb_rnd.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)\n",
    "save_data_configuration(f\"# Training duration: {time.time()-time_start}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_profiler:Monitor runs begin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tb = tensorboard.program.TensorBoard()\n",
    "tb.configure(bind_all=True, logdir=f\"4_Modelling/{logs_base_dir}\")\n",
    "url = tb.launch()\n",
    "webbrowser.open_new_tab(url.replace(\"MSI\",\"localhost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7452), started 0:23:16 ago. (Use '!kill 7452' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8bc7cf743a1a222a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8bc7cf743a1a222a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17036), started 0:23:14 ago. (Use '!kill 17036' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c4d4614233976ec8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c4d4614233976ec8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorboard:Deleting accumulator 'runs_StratifiedSamplingByUsers_RANDOM'\n",
      "WARNING:tensorboard:Deleting accumulator 'runs_StratifiedSamplingByUsers_FM'\n",
      "WARNING:tensorboard:Deleting accumulator 'runs_StratifiedSamplingByUsers_RANDOM'\n",
      "WARNING:tensorboard:Deleting accumulator 'runs_StratifiedSamplingByUsers_FM'\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir run_tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977d1ca4f86cf6796d22eaa6050e2f72f1e03740aa8986f601710bf090b5ff9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
